{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Environment Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. *** FIX: Clear problematic environment variable for matplotlib ***\n",
        "# This prevents the \"ValueError: Key backend: 'module://matplotlib_inline.backend_inline'\" error\n",
        "if 'MPLBACKEND' in os.environ:\n",
        "    del os.environ['MPLBACKEND']\n",
        "    print(\"MPLBACKEND environment variable cleared.\")\n",
        "\n",
        "# 2. Clone the repository\n",
        "!rm -rf FontDiffusion\n",
        "!git clone https://github.com/dzungphieuluuky/FontDiffusion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFvN9XJxf9K",
        "outputId": "7e1e2be4-8da4-4736-f9c3-5ebb8fa9ad2a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:41:44.378903Z",
          "iopub.execute_input": "2025-12-28T10:41:44.379383Z",
          "iopub.status.idle": "2025-12-28T10:42:10.309903Z",
          "shell.execute_reply.started": "2025-12-28T10:41:44.379353Z",
          "shell.execute_reply": "2025-12-28T10:42:10.308650Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FontDiffusion'...\n",
            "remote: Enumerating objects: 15148, done.\u001b[K\n",
            "remote: Counting objects: 100% (2961/2961), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2868/2868), done.\u001b[K\n",
            "remote: Total 15148 (delta 138), reused 2898 (delta 91), pack-reused 12187 (from 3)\u001b[K\n",
            "Receiving objects: 100% (15148/15148), 247.43 MiB | 33.43 MiB/s, done.\n",
            "Resolving deltas: 100% (526/526), done.\n",
            "Updating files: 100% (128/128), done.\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install --upgrade pip\n",
        "!uv pip install -r FontDiffusion/requirements.txt\n",
        "!uv pip install gdown\n",
        "# 3. Install PyTorch 1.13\n",
        "print(\"\\n‚¨áÔ∏è Installing PyTorch 1.13 (Required for this model)...\")\n",
        "# Force reinstall torch 1.13 to match the model's training environment\n",
        "!uv pip uninstall torch torchvision\n",
        "!uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "# 4. Install other dependencies\n",
        "print(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n",
        "# Install xformers compatible with Torch 1.13\n",
        "!uv pip install xformers==0.0.16 -q\n",
        "\n",
        "# Install original dependencies\n",
        "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
        "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
        "# -----------------------------------------------------------------\n",
        "!uv pip install lpips scikit-image pytorch-fid\n",
        "!sudo apt-get update && sudo apt-get install dos2unix\n",
        "print(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET_mqyek9bwj",
        "outputId": "af2489e8-76f8-4488-c78e-d6718b784892"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[37m‚†ã\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ã\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mpip==25.3                                                                     \u001b[0m\r\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 35ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.20ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m116 packages\u001b[0m \u001b[2min 201ms\u001b[0m\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `tokenizers==0.13.3`\n",
            "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
            "\u001b[31m      \u001b[0mrunning bdist_wheel\n",
            "\u001b[31m      \u001b[0mrunning build\n",
            "\u001b[31m      \u001b[0mrunning build_py\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mrunning build_ext\n",
            "\u001b[31m      \u001b[0mrunning build_rust\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
            "\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpZTb10l/lib/python3.12/site-packages/setuptools/dist.py:759:\n",
            "\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n",
            "\u001b[31m      \u001b[0mSPDX license expression:\n",
            "\n",
            "\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n",
            "\n",
            "\u001b[31m      \u001b[0m        See\n",
            "\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n",
            "\u001b[31m      \u001b[0mfor details.\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\u001b[31m      \u001b[0m  self._finalize_license_expression()\n",
            "\u001b[31m      \u001b[0merror: can't find Rust compiler\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n",
            "\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n",
            "\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "\n",
            "\u001b[31m      \u001b[0mTo update pip, run:\n",
            "\n",
            "\u001b[31m      \u001b[0m    pip install --upgrade pip\n",
            "\n",
            "\u001b[31m      \u001b[0mand then retry package installation.\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n",
            "\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n",
            "\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n",
            "\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n",
            "\u001b[31m      \u001b[0mRust compiler toolchain.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
            "\u001b[31m      \u001b[0menvironment.\n",
            "\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n",
            "        depends on `\u001b[36mtokenizers\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 146ms\u001b[0m\u001b[0m\n",
            "\n",
            "‚¨áÔ∏è Installing PyTorch 1.13 (Required for this model)...\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 306ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause torch==1.13.1+cu117 has no wheels with a matching Python ABI\n",
            "\u001b[31m      \u001b[0mtag (e.g., `\u001b[36mcp312\u001b[39m`) and you require torch==1.13.1+cu117, we can conclude\n",
            "\u001b[31m      \u001b[0mthat your requirements are unsatisfiable.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m `\u001b[36mtorch\u001b[39m` was found on \u001b[36mhttps://download.pytorch.org/whl/cu117\u001b[39m, but\n",
            "\u001b[31m      \u001b[0mnot at the requested version (\u001b[36mtorch==1.13.1+cu117\u001b[39m). A compatible version\n",
            "\u001b[31m      \u001b[0mmay be available on a subsequent index (e.g., \u001b[36mhttps://pypi.org/simple\u001b[39m).\n",
            "\u001b[31m      \u001b[0mBy default, uv will only consider versions that are published on the\n",
            "\u001b[31m      \u001b[0mfirst index that contains a given package, to avoid dependency confusion\n",
            "\u001b[31m      \u001b[0mattacks. If all indexes are equally trusted, use `\u001b[32m--index-strategy\n",
            "\u001b[31m      \u001b[0munsafe-best-match\u001b[39m` to consider all versions from all indexes, regardless\n",
            "\u001b[31m      \u001b[0mof the order in which they were defined.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n",
            "\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1+cu117\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`,\n",
            "\u001b[31m      \u001b[0m`\u001b[36mcp38\u001b[39m`, `\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n",
            "\n",
            "‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n",
            "  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause torch==1.13.1 has no wheels with a matching Python ABI tag\n",
            "\u001b[31m      \u001b[0m(e.g., `\u001b[36mcp312\u001b[39m`) and xformers==0.0.16 depends on torch==1.13.1, we can\n",
            "\u001b[31m      \u001b[0mconclude that xformers==0.0.16 cannot be used.\n",
            "\u001b[31m      \u001b[0mAnd because you require xformers==0.0.16, we can conclude that your\n",
            "\u001b[31m      \u001b[0mrequirements are unsatisfiable.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n",
            "\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`, `\u001b[36mcp38\u001b[39m`,\n",
            "\u001b[31m      \u001b[0m`\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `tokenizers==0.13.3`\n",
            "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
            "\u001b[31m      \u001b[0mrunning bdist_wheel\n",
            "\u001b[31m      \u001b[0mrunning build\n",
            "\u001b[31m      \u001b[0mrunning build_py\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mrunning build_ext\n",
            "\u001b[31m      \u001b[0mrunning build_rust\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
            "\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpi2pjSU/lib/python3.12/site-packages/setuptools/dist.py:759:\n",
            "\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n",
            "\u001b[31m      \u001b[0mSPDX license expression:\n",
            "\n",
            "\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n",
            "\n",
            "\u001b[31m      \u001b[0m        See\n",
            "\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n",
            "\u001b[31m      \u001b[0mfor details.\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\u001b[31m      \u001b[0m  self._finalize_license_expression()\n",
            "\u001b[31m      \u001b[0merror: can't find Rust compiler\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n",
            "\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n",
            "\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "\n",
            "\u001b[31m      \u001b[0mTo update pip, run:\n",
            "\n",
            "\u001b[31m      \u001b[0m    pip install --upgrade pip\n",
            "\n",
            "\u001b[31m      \u001b[0mand then retry package installation.\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n",
            "\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n",
            "\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n",
            "\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n",
            "\u001b[31m      \u001b[0mRust compiler toolchain.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
            "\u001b[31m      \u001b[0menvironment.\n",
            "\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n",
            "        depends on `\u001b[36mtokenizers\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 80ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 292ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.20.0` does not have an extra named `all`\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m38 packages\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "dos2unix is already the newest version (7.4.2-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "\n",
            "‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "\n",
        "def configure_environment_paths():\n",
        "    \"\"\"Detect environment and configure paths\"\"\"\n",
        "    try:\n",
        "        if \"google.colab\" in str(get_ipython()):\n",
        "            print(\"‚úÖ Environment: Google Colab\")\n",
        "            base_data_path = \"/content/\"\n",
        "            base_output_path = \"/content/\"\n",
        "            environment_name = \"colab\"\n",
        "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
        "            print(\"‚úÖ Environment: Kaggle\")\n",
        "            base_data_path = \"/kaggle/input/\"\n",
        "            base_output_path = \"/kaggle/working/\"\n",
        "            environment_name = \"kaggle\"\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
        "            base_data_path = \"./data/\"\n",
        "            base_output_path = \"./output/\"\n",
        "            environment_name = \"local\"\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
        "        base_data_path = \"./data/\"\n",
        "        base_output_path = \"./output/\"\n",
        "        environment_name = \"local\"\n",
        "\n",
        "    os.makedirs(base_output_path, exist_ok=True)\n",
        "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
        "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
        "\n",
        "    return base_data_path, base_output_path, environment_name\n",
        "\n",
        "\n",
        "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxdyquWfaqdm",
        "outputId": "044d781a-84a8-476f-f5dd-d07895c78c17",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:29:48.372367Z",
          "iopub.execute_input": "2025-12-28T10:29:48.372873Z",
          "iopub.status.idle": "2025-12-28T10:29:48.380165Z",
          "shell.execute_reply.started": "2025-12-28T10:29:48.372846Z",
          "shell.execute_reply": "2025-12-28T10:29:48.379601Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment: Google Colab\n",
            "üìÇ Data Path: /content/\n",
            "üì¶ Output Path: /content/\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "if \"colab\" in ENV_NAME:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        # Ensure 'WANDB_API_KEY' is the exact name in your Colab Secrets (the key icon)\n",
        "        wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Colab Secrets: {e}\")\n",
        "\n",
        "# 2. Check if running in Kaggle\n",
        "elif \"kaggle\" in ENV_NAME:\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "        user_secrets = UserSecretsClient()\n",
        "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Kaggle Secrets: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h06w314Jaqdm",
        "outputId": "144b1dfa-ede1-47b2-d4ff-dd213f45e048",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:29:48.381856Z",
          "iopub.execute_input": "2025-12-28T10:29:48.382150Z",
          "iopub.status.idle": "2025-12-28T10:29:50.899296Z",
          "shell.execute_reply.started": "2025-12-28T10:29:48.382126Z",
          "shell.execute_reply": "2025-12-28T10:29:50.898291Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "if not os.path.exists(\"ckpt\"):\n",
        "  url = \"https://drive.google.com/drive/folders/12hfuZ9MQvXqcteNuz7JQ2B_mUcTr-5jZ\"\n",
        "  gdown.download_folder(url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "id": "9PsLgUs0cYmO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:29:50.900151Z",
          "iopub.execute_input": "2025-12-28T10:29:50.900515Z",
          "iopub.status.idle": "2025-12-28T10:30:11.203927Z",
          "shell.execute_reply.started": "2025-12-28T10:29:50.900494Z",
          "shell.execute_reply": "2025-12-28T10:30:11.203305Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unzipping all archived files\n",
        "import os\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n",
        "\n",
        "if not zip_file_paths:\n",
        "    print(f'No .zip files found in {INPUT_PATH}.')\n",
        "else:\n",
        "    for zip_file_path in zip_file_paths:\n",
        "        if os.path.exists(zip_file_path):\n",
        "            print(f'Unzipping {zip_file_path}...')\n",
        "            !unzip -q -o {zip_file_path} -d ./\n",
        "            print(f'Unzipping of {zip_file_path} complete.')\n",
        "        else:\n",
        "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfc18e0",
        "outputId": "c64a1a7b-f407-4abc-aa30-8f3301594680",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:11.205802Z",
          "iopub.execute_input": "2025-12-28T10:30:11.206149Z",
          "iopub.status.idle": "2025-12-28T10:30:11.212595Z",
          "shell.execute_reply.started": "2025-12-28T10:30:11.206130Z",
          "shell.execute_reply": "2025-12-28T10:30:11.211927Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No .zip files found in /content/.\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checking checkpoint files (.pth)\n",
        "import os\n",
        "import time\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(INPUT_PATH, \"ckpt\")\n",
        "print(CHECKPOINT_DIR)\n",
        "# Create the checkpoint directory\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "# Wait loop to check if files exist\n",
        "required_files = [\"unet.pth\", \"content_encoder.pth\", \"style_encoder.pth\"]\n",
        "\n",
        "while True:\n",
        "    missing = [f for f in required_files if not os.path.exists(f\"{CHECKPOINT_DIR}/{f}\")]\n",
        "\n",
        "    if not missing:\n",
        "        print(\"\\n‚úÖ All weights found! You can proceed to the next step.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Waiting for files... Missing: {missing}\")\n",
        "        print(\"Upload them to the 'ckpt' folder now.\")\n",
        "        time.sleep(10) # Checks every 10 seconds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBflCTABxlF4",
        "outputId": "8ead15f6-305a-4a7a-f477-4c8eae204bee",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:11.213524Z",
          "iopub.execute_input": "2025-12-28T10:30:11.213846Z",
          "iopub.status.idle": "2025-12-28T10:30:11.248712Z",
          "shell.execute_reply.started": "2025-12-28T10:30:11.213822Z",
          "shell.execute_reply": "2025-12-28T10:30:11.247563Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ckpt\n",
            "\n",
            "‚úÖ All weights found! You can proceed to the next step.\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n",
        "    \"\"\"\n",
        "    Reads a CSV file, extracts text from a specified column, and writes each character\n",
        "    to a new line in a plain text file.\n",
        "\n",
        "    Args:\n",
        "        input_csv_path (str): The full path to the input CSV file.\n",
        "        output_txt_path (str): The full path for the output text file.\n",
        "        column_name (str): The name of the column in the CSV file containing the text.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_csv_path):\n",
        "        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n",
        "        return\n",
        "\n",
        "    if column_name not in df.columns:\n",
        "        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n",
        "        return\n",
        "\n",
        "    all_characters = []\n",
        "    # Ensure the column values are treated as strings before iterating over them\n",
        "    for item in df[column_name].astype(str).dropna().tolist():\n",
        "        for char in item:\n",
        "            all_characters.append(char)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
        "\n",
        "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(all_characters))\n",
        "    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\n",
        "\n",
        "# --- Example Usage (demonstration with a dummy file) ---\n",
        "# As the original file 'Ds_300_ChuNom_TuTao.csv' was not found in the previous execution,\n",
        "# let's create a dummy file to demonstrate the function's usage.\n",
        "print(\"\\n--- Demonstrating function with a dummy CSV file ---\")\n",
        "dummy_csv_path = os.path.join(INPUT_PATH, \"dummy_data.csv\")\n",
        "dummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\n",
        "\n",
        "# Create a dummy CSV file\n",
        "dummy_data = {'word': ['hello', 'world', 'python']}\n",
        "pd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\n",
        "print(f\"Created a dummy CSV file at: {dummy_csv_path}\")\n",
        "\n",
        "convert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)\n",
        "\n",
        "# --- How to use with your actual file ---\n",
        "# Uncomment the lines below and replace 'your_actual_file.csv' and 'your_output.txt'\n",
        "# with the correct paths for your use case.\n",
        "#\n",
        "# original_csv_file = os.path.join(INPUT_PATH, \"Ds_300_ChuNom_TuTao.csv\") # Or the full path to your CSV\n",
        "# original_output_txt = os.path.join(OUTPUT_PATH, \"nom_tu_tao.txt\") # Or your desired output path\n",
        "# convert_csv_to_chars_txt(original_csv_file, original_output_txt)\n"
      ],
      "metadata": {
        "id": "Mx5uS5WQaqdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cabd97a-ea23-426a-c463-345beb1bd8a0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:42.952259Z",
          "iopub.execute_input": "2025-12-28T10:30:42.953132Z",
          "iopub.status.idle": "2025-12-28T10:30:43.328221Z",
          "shell.execute_reply.started": "2025-12-28T10:30:42.953098Z",
          "shell.execute_reply": "2025-12-28T10:30:43.326792Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Demonstrating function with a dummy CSV file ---\n",
            "Created a dummy CSV file at: /content/dummy_data.csv\n",
            "Successfully converted '/content/dummy_data.csv' to '/content/dummy_chars.txt', with one character per line.\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {OUTPUT_PATH}/FontDiffusion\n",
        "!python sample_batch.py \\\n",
        "    --characters \"NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n",
        "    --start_line 201 \\\n",
        "    --end_line 500 \\\n",
        "    --style_images \"/content/FontDiffusion/styles_images\" \\\n",
        "    --ckpt_dir \"../ckpt/\" \\\n",
        "    --ttf_path \"fonts/NomNaTong-Regular.otf\" \\\n",
        "    --output_dir \"../my_dataset\" \\\n",
        "    --batch_size 24 \\\n",
        "    --save_interval 5 \\\n",
        "    --channels_last \\\n",
        "    --num_inference_steps 20 \\\n",
        "    --guidance_scale 7.5 \\\n",
        "    --seed 42 \\\n",
        "    --compile \\\n",
        "    --enable_xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gma02BZvhx8I",
        "outputId": "2e01df1d-7312-4564-a825-1b9566c2db2b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:42:25.671647Z",
          "iopub.execute_input": "2025-12-28T10:42:25.672117Z",
          "iopub.status.idle": "2025-12-28T10:42:25.799293Z",
          "shell.execute_reply.started": "2025-12-28T10:42:25.672076Z",
          "shell.execute_reply": "2025-12-28T10:42:25.798554Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FontDiffusion\n",
            "pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "\n",
            "============================================================\n",
            "FONTDIFFUSER STANDARD FORMAT GENERATION\n",
            "============================================================\n",
            "Loading characters from lines 201 to 500 (total: 10174 lines)\n",
            "Successfully loaded 300 single characters.\n",
            "\n",
            "Initializing font manager...\n",
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
            "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
            "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
            "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
            "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
            "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
            "‚úì Loaded font: NomNaTong-Regular\n",
            "\n",
            "üìä Configuration:\n",
            "  Characters: 300 (lines 201-500)\n",
            "  Styles: 15\n",
            "  Output: ../my_dataset\n",
            "\n",
            "============================================================\n",
            "Generating Content Images\n",
            "Using font: NomNaTong-Regular\n",
            "Characters: 300\n",
            "============================================================\n",
            "  ‚ö† Warning: '†µô' not in font, skipping...\n",
            "üìù Content images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:25<00:00, 11.95it/s]\n",
            "‚úì Generated 299 content images\n",
            "============================================================\n",
            "\n",
            "Loading FontDiffuser pipeline...\n",
            "Loading FontDiffuser pipeline...\n",
            "Load the down block  DownBlock2D\n",
            "Load the down block  MCADownBlock2D\n",
            "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
            "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
            "Load the down block  MCADownBlock2D\n",
            "The style_attention cross attention dim in Down Block 1 layer is 1024\n",
            "The style_attention cross attention dim in Down Block 2 layer is 1024\n",
            "Load the down block  DownBlock2D\n",
            "Load the up block  UpBlock2D\n",
            "Load the up block  StyleRSIUpBlock2D\n",
            "Load the up block  StyleRSIUpBlock2D\n",
            "Load the up block  UpBlock2D\n",
            "Param count for Ds initialized parameters: 20591296\n",
            "Get CG-GAN Style Encoder!\n",
            "Param count for Ds initialized parameters: 1187008\n",
            "Get CG-GAN Content Encoder!\n",
            "‚úì Loaded model state_dict successfully\n",
            "Converting to channels-last memory format...\n",
            "2025-12-28 18:01:03.414264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766944863.437842   10534 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766944863.443682   10534 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766944863.459121   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766944863.459147   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766944863.459153   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766944863.459157   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-28 18:01:03.463105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "‚úì Model moved to device\n",
            "‚úì Loaded training DDPM scheduler successfully\n",
            "‚úì Loaded DPM-Solver pipeline successfully\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "\n",
            "============================================================\n",
            "BATCH GENERATION\n",
            "============================================================\n",
            "Fonts: 1\n",
            "Styles: 15\n",
            "Characters: 300\n",
            "Batch size: 24\n",
            "Inference steps: 20\n",
            "Save interval: 5\n",
            "============================================================\n",
            "\n",
            "Using font: NomNaTong-Regular\n",
            "Available characters: 299/300\n",
            "üé® Generating styles:   0%|                               | 0/15 [00:00<?, ?it/s, Processing style0]/content/FontDiffusion/src/model.py:88: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n",
            "  style_img_feature, _, style_residual_features = self.style_encoder(style_images)\n",
            "/content/FontDiffusion/src/model.py:94: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n",
            "  content_img_feture, content_residual_features = self.content_encoder(content_images)\n",
            "/content/FontDiffusion/src/model.py:97: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n",
            "  style_content_feature, style_content_res_features = self.content_encoder(style_images)\n",
            "/content/FontDiffusion/src/model.py:102: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n",
            "  out = self.unet(\n",
            "  ‚úì style0: 299 images in 483.06s (1.616s/img)\n",
            "  ‚úì style1: 299 images in 485.88s (1.625s/img)\n",
            "  ‚úì style2: 299 images in 485.83s (1.625s/img)\n",
            "üé® Generating styles:  20%|‚ñà‚ñà‚ñà‚ñà                | 3/15 [28:54<1:55:39, 578.25s/it, Processing style3]\n",
            "\n",
            "\n",
            "‚ö† Generation interrupted by user!\n",
            "üíæ Saving checkpoint before exit...\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "from typing import List\n",
        "def find_result_folders(base_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Return a list of absolute paths to all directories under `base_path`\n",
        "    whose names start with 'results_'.\n",
        "    \"\"\"\n",
        "    pattern = os.path.join(base_path, \"*dataset\")\n",
        "    # glob returns both files and directories; filter to directories only\n",
        "    return [p for p in glob.glob(pattern) if os.path.isdir(p)]\n",
        "def zip_folder(folder_path: str, output_base_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Zip the contents of `folder_path` into a file named\n",
        "    <folder_name>.zip` inside `output_base_path`.\n",
        "\n",
        "    Returns True on success, False otherwise.\n",
        "    \"\"\"\n",
        "    folder_name = os.path.basename(folder_path)\n",
        "    zip_path = os.path.join(output_base_path, f\"{folder_name}.zip\")\n",
        "    try:\n",
        "        print(f\"   -> Zipping folder: {folder_name}...\")\n",
        "        with zipfile.ZipFile(\n",
        "            zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED\n",
        "        ) as zipf:\n",
        "            for root, _, files in os.walk(folder_path):\n",
        "                for file in files:\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    # Preserve relative path inside the zip\n",
        "                    arcname = os.path.relpath(full_path, os.path.dirname(folder_path))\n",
        "                    zipf.write(full_path, arcname)\n",
        "        print(f\"   ‚úÖ Created ZIP: {os.path.basename(zip_path)}\")\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n",
        "        return False\n",
        "def zip_stats_results_folders(output_base_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Main driver: locate all result folders and zip each one.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_base_path, exist_ok=True)\n",
        "    result_folders = find_result_folders(output_base_path)\n",
        "    if not result_folders:\n",
        "        print(f\"‚ö†Ô∏è No folders starting with 'results_' found in '{output_base_path}'.\")\n",
        "        return\n",
        "    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n",
        "    successful = 0\n",
        "    for folder in result_folders:\n",
        "        if zip_folder(folder, output_base_path):\n",
        "            successful += 1\n",
        "    print(\n",
        "        f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\"\n",
        "    )\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Prefer an environment variable; fall back to a global if defined\n",
        "        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n",
        "        if not output_root:\n",
        "            raise ValueError(\"OUTPUT_PATH not defined\")\n",
        "        # The script expects a sub‚Äëfolder named 'OuroTrace' under OUTPUT_PATH\n",
        "        target_path = os.path.join(output_root, \"\")\n",
        "        zip_stats_results_folders(target_path)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "kTz9WZ9ylBZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2c1be7-6400-4b14-91d0-47804c3896fe",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:43.330470Z",
          "iopub.status.idle": "2025-12-28T10:30:43.330877Z",
          "shell.execute_reply.started": "2025-12-28T10:30:43.330672Z",
          "shell.execute_reply": "2025-12-28T10:30:43.330691Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Found 1 result folder(s) to zip.\n",
            "   -> Zipping folder: my_dataset...\n",
            "   ‚úÖ Created ZIP: my_dataset.zip\n",
            "\n",
            "‚úÖ DONE! Successfully zipped 1 out of 1 folder(s).\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Happy Christmas‚ú®\n",
        "# !rm -r -f FontDiffusion"
      ],
      "metadata": {
        "id": "SIH9c0l-mRqB",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-28T10:30:43.331996Z",
          "iopub.status.idle": "2025-12-28T10:30:43.332314Z",
          "shell.execute_reply.started": "2025-12-28T10:30:43.332143Z",
          "shell.execute_reply": "2025-12-28T10:30:43.332159Z"
        }
      },
      "outputs": [],
      "execution_count": 23
    }
  ]
}