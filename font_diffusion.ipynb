{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":127.636509,"end_time":"2025-12-30T18:55:25.961447","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-30T18:53:18.324938","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a95a46ef","cell_type":"code","source":"# @title Environment Setup\nimport os\nimport sys\nif 'MPLBACKEND' in os.environ:\n    del os.environ['MPLBACKEND']\n    print(\"MPLBACKEND environment variable cleared.\")\n\n# 2. Clone the repository\n!rm -rf FontDiffusion\n!git clone https://github.com/dzungphieuluuky/FontDiffusion.git","metadata":{"id":"a95a46ef","outputId":"d76d28cd-6292-42bf-fffa-a8c7efb86ed0","papermill":{"duration":12.857369,"end_time":"2025-12-30T18:53:35.066181","exception":false,"start_time":"2025-12-30T18:53:22.208812","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:09:51.354653Z","iopub.execute_input":"2025-12-31T12:09:51.355249Z","iopub.status.idle":"2025-12-31T12:10:02.853407Z","shell.execute_reply.started":"2025-12-31T12:09:51.355216Z","shell.execute_reply":"2025-12-31T12:10:02.852480Z"},"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'FontDiffusion'...\n","remote: Enumerating objects: 20320, done.\u001b[K\n","remote: Counting objects: 100% (83/83), done.\u001b[K\n","remote: Compressing objects: 100% (60/60), done.\u001b[K\n","remote: Total 20320 (delta 46), reused 52 (delta 23), pack-reused 20237 (from 1)\u001b[K\n","Receiving objects: 100% (20320/20320), 277.43 MiB | 35.08 MiB/s, done.\n","Resolving deltas: 100% (778/778), done.\n","Updating files: 100% (137/137), done.\n"]}],"execution_count":20},{"id":"9cdd8666","cell_type":"code","source":"import os\nimport sys\nfrom IPython import get_ipython\nfrom typing import Optional\n\ndef configure_environment_paths():\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"âœ… Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"âœ… Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"âš ï¸ Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"âš ï¸ Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"ðŸ“‚ Data Path: {base_data_path}\")\n    print(f\"ðŸ“¦ Output Path: {base_output_path}\")\n    return base_data_path, base_output_path, environment_name\n\ndef load_secret(key_name: str) -> Optional[str]:\n    env = ENV_NAME\n    secret_value = None\n    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n    try:\n        if env == \"colab\":\n            from google.colab import userdata\n            secret_value = userdata.get(key_name)\n        elif env == \"kaggle\":\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            secret_value = user_secrets.get_secret(key_name)\n        else:\n            secret_value = os.getenv(key_name)\n        if not secret_value:\n            print(f\"âš ï¸ Secret '{key_name}' not found in the {env} environment.\")\n            return None\n        print(f\"âœ… Successfully loaded secret '{key_name}'.\")\n        return secret_value\n    except Exception as e:\n        print(f\"âŒ An error occurred while loading secret '{key_name}': {e}\")\n        return None\n\ndef print_system_info():\n    print(\"\\nðŸ”§ System Information\")\n    print(f\"Python version: {sys.version.split()[0]}\")\n    try:\n        import torch\n        print(f\"PyTorch version: {torch.__version__}\")\n        if torch.cuda.is_available():\n            print(f\"CUDA version: {torch.version.cuda}\")\n            print(f\"GPU count: {torch.cuda.device_count()}\")\n            for i in range(torch.cuda.device_count()):\n                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        else:\n            print(\"CUDA not available\")\n    except ImportError:\n        print(\"PyTorch not installed\")\n    finally:\n      !nvidia-smi\n\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\nis_kaggle = (\"kaggle\" in ENV_NAME)\nis_colab = not is_kaggle\nprint_system_info()","metadata":{"id":"9cdd8666","outputId":"8834f4e4-fc28-455c-a66c-d15b00de080a","papermill":{"duration":0.019157,"end_time":"2025-12-30T18:53:35.092303","exception":false,"start_time":"2025-12-30T18:53:35.073146","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Environment: Google Colab\n","ðŸ“‚ Data Path: /content/\n","ðŸ“¦ Output Path: /content/\n","\n","ðŸ”§ System Information\n","Python version: 3.12.12\n","PyTorch version: 2.9.0+cu126\n","CUDA version: 12.6\n","GPU count: 1\n","  GPU 0: Tesla T4\n","Wed Dec 31 16:28:33 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"execution_count":4},{"id":"a73b4150","cell_type":"code","source":"!uv pip install --upgrade pip\n# 3. Install PyTorch 1.13\n%cd {OUTPUT_PATH}\n# Force reinstall torch 1.13 to match the model's training environment\n# !uv pip uninstall torch torchvision\n# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!uv pip install torch torchvision\n# 4. Install other dependencies\nprint(\"\\nâ¬‡ï¸ Installing Dependencies (Manually fixed)...\")\n# Install xformers compatible with Torch 1.13\n!uv pip install xformers==0.0.16 -q\n\n# Install original dependencies\n!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n# -----------------------------------------------------------------\n!uv pip install lpips scikit-image pytorch-fid\n!sudo apt-get update && sudo apt-get install dos2unix\n!uv pip install gdown\n!uv pip install wandb\n!uv pip install --upgrade pyarrow datasets\nprint(\"\\nâœ… Environment setup complete. You can now proceed to Block 2 (Inference).\")","metadata":{"id":"a73b4150","outputId":"97db2cec-8e2d-438b-e5f8-38df08b7f59e","papermill":{"duration":61.239828,"end_time":"2025-12-30T18:54:36.338205","exception":false,"start_time":"2025-12-30T18:53:35.098377","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpip==25.3                                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 26ms\u001b[0m\u001b[0m\n","\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.19ms\u001b[0m\u001b[0m\n","/content\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 84ms\u001b[0m\u001b[0m\n","\n","â¬‡ï¸ Installing Dependencies (Manually fixed)...\n","  \u001b[31mÃ—\u001b[0m No solution found when resolving dependencies:\n","\u001b[31m  â•°â”€â–¶ \u001b[0mBecause torch==1.13.1 has no wheels with a matching Python ABI tag\n","\u001b[31m      \u001b[0m(e.g., `\u001b[36mcp312\u001b[39m`) and xformers==0.0.16 depends on torch==1.13.1, we can\n","\u001b[31m      \u001b[0mconclude that xformers==0.0.16 cannot be used.\n","\u001b[31m      \u001b[0mAnd because you require xformers==0.0.16, we can conclude that your\n","\u001b[31m      \u001b[0mrequirements are unsatisfiable.\n","\n","\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n","\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`, `\u001b[36mcp38\u001b[39m`,\n","\u001b[31m      \u001b[0m`\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 89ms\u001b[0m\u001b[0m\n","\u001b[2K  \u001b[31mÃ—\u001b[0m Failed to build `tokenizers==0.13.3`\n","\u001b[31m  â”œâ”€â–¶ \u001b[0mThe build backend returned an error\n","\u001b[31m  â•°â”€â–¶ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n","\n","\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n","\u001b[31m      \u001b[0mrunning bdist_wheel\n","\u001b[31m      \u001b[0mrunning build\n","\u001b[31m      \u001b[0mrunning build_py\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n","\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n","\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n","\u001b[31m      \u001b[0mrunning build_ext\n","\u001b[31m      \u001b[0mrunning build_rust\n","\n","\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n","\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmp1iIIdz/lib/python3.12/site-packages/setuptools/dist.py:759:\n","\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n","\u001b[31m      \u001b[0m!!\n","\n","\u001b[31m      \u001b[0m\n","\u001b[31m      \u001b[0m********************************************************************************\n","\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n","\u001b[31m      \u001b[0mSPDX license expression:\n","\n","\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n","\n","\u001b[31m      \u001b[0m        See\n","\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n","\u001b[31m      \u001b[0mfor details.\n","\u001b[31m      \u001b[0m\n","\u001b[31m      \u001b[0m********************************************************************************\n","\n","\u001b[31m      \u001b[0m!!\n","\u001b[31m      \u001b[0m  self._finalize_license_expression()\n","\u001b[31m      \u001b[0merror: can't find Rust compiler\n","\n","\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n","\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n","\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n","\n","\u001b[31m      \u001b[0mTo update pip, run:\n","\n","\u001b[31m      \u001b[0m    pip install --upgrade pip\n","\n","\u001b[31m      \u001b[0mand then retry package installation.\n","\n","\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n","\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n","\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n","\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n","\u001b[31m      \u001b[0mRust compiler toolchain.\n","\n","\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n","\u001b[31m      \u001b[0menvironment.\n","\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n","        depends on `\u001b[36mtokenizers\u001b[39m`\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m92 packages\u001b[0m \u001b[2min 170ms\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n","\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.20.0` does not have an extra named `all`\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 94ms\u001b[0m\u001b[0m\n","Hit:1 https://cli.github.com/packages stable InRelease\n","Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Fetched 384 kB in 1s (319 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","dos2unix is already the newest version (7.4.2-2).\n","0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m\n","\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 140ms\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 0.59ms\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 18ms\u001b[0m\u001b[0m\n","\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 28ms\u001b[0m\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n","\n","âœ… Environment setup complete. You can now proceed to Block 2 (Inference).\n"]}],"execution_count":5},{"id":"bd517dfe","cell_type":"code","source":"# KAGGLE CELL #1: Download checkpoint\n\nimport os\nimport sys\nfrom pathlib import Path\nos.chdir(OUTPUT_PATH)\n# Download from Hub\nif not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n    print(\"ðŸ“¥ Downloading checkpoint from Hugging Face Hub...\\n\")\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id=\"dzungpham/font-diffusion-weights\",\n        local_dir=\"ckpt\",\n        allow_patterns=\"*.safetensors\",\n        force_download=False\n    )\n    print(\"\\nâœ… Download complete!\")\nelse:\n    print(\"âœ… Checkpoint already downloaded\")\n# Verify\nprint(\"\\nðŸ“‚ Files in ckpt/:\")\nfor file in os.listdir(\"ckpt\"):\n    if file.endswith(\".safetensors\"):\n        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n        print(f\"  âœ“ {file} ({size:.2f} MB)\")","metadata":{"id":"bd517dfe","outputId":"d83605e9-f5dc-4862-d1c9-b138a96ca47a","papermill":{"duration":12.524295,"end_time":"2025-12-30T18:54:48.878013","exception":false,"start_time":"2025-12-30T18:54:36.353718","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Checkpoint already downloaded\n","\n","ðŸ“‚ Files in ckpt/:\n","  âœ“ unet.safetensors (300.34 MB)\n","  âœ“ content_encoder.safetensors (4.54 MB)\n","  âœ“ style_encoder.safetensors (78.58 MB)\n"]}],"execution_count":6},{"id":"767e8ea2","cell_type":"code","source":"# @title Unzipping all archived files\nimport os\nimport glob\nfrom zipfile import ZipFile\n\nzip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n\nif not zip_file_paths:\n    print(f'No .zip files found in {INPUT_PATH}.')\nelse:\n    for zip_file_path in zip_file_paths:\n        if os.path.exists(zip_file_path):\n            print(f'Unzipping {zip_file_path}...')\n            !unzip -o {zip_file_path} -d ./\n            print(f'Unzipping of {zip_file_path} complete.')\n        else:\n            print(f'Error: The file {zip_file_path} was not found (post-glob check).')","metadata":{"id":"767e8ea2","outputId":"20185e27-e772-4823-e6bc-d9bd6d0b39a1","papermill":{"duration":0.023805,"end_time":"2025-12-30T18:54:48.917163","exception":false,"start_time":"2025-12-30T18:54:48.893358","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["No .zip files found in /content/.\n"]}],"execution_count":7},{"id":"51941368","cell_type":"code","source":"import pandas as pd\nimport os\ndef convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n    \"\"\"\n    Reads a CSV file, extracts text from a specified column, and writes each character\n    to a new line in a plain text file.\n    Args:\n        input_csv_path (str): The full path to the input CSV file.\n        output_txt_path (str): The full path for the output text file.\n        column_name (str): The name of the column in the CSV file containing the text.\n    \"\"\"\n    if not os.path.exists(input_csv_path):\n        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n        return\n    try:\n        df = pd.read_csv(input_csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n        return\n    if column_name not in df.columns:\n        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n        return\n    all_characters = []\n    for item in df[column_name].astype(str).dropna().tolist():\n        for char in item:\n            all_characters.append(char)\n    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(all_characters))\n    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\nprint(\"\\n--- Demonstrating function with a dummy CSV file ---\")\ndummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\ndummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\ndummy_data = {'word': ['hello', 'world', 'python']}\npd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\nprint(f\"Created a dummy CSV file at: {dummy_csv_path}\")\nconvert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)","metadata":{"id":"51941368","outputId":"2a2c352c-968a-4e4d-b4cc-88a02c7eb788","papermill":{"duration":1.62157,"end_time":"2025-12-30T18:54:50.594793","exception":false,"start_time":"2025-12-30T18:54:48.973223","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Demonstrating function with a dummy CSV file ---\n","Created a dummy CSV file at: /content/dummy_data.csv\n","Successfully converted '/content/dummy_data.csv' to '/content/dummy_chars.txt', with one character per line.\n"]}],"execution_count":8},{"id":"4f4cf20b","cell_type":"code","source":"!ls -larth {OUTPUT_PATH}/ckpt\nfrom huggingface_hub import login\nHF_TOKEN = load_secret(\"HF_TOKEN\")\nlogin(HF_TOKEN)\nHF_USERNAME = \"dzungpham\"","metadata":{"id":"4f4cf20b","outputId":"335f4192-47e7-451a-e14f-e0bd69fbdfc9","papermill":{"duration":0.140282,"end_time":"2025-12-30T18:54:50.749810","exception":false,"start_time":"2025-12-30T18:54:50.609528","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["total 384M\n","drwxr-xr-x 3 root root 4.0K Dec 31 14:57 .cache\n","-rw-r--r-- 1 root root 4.6M Dec 31 14:57 content_encoder.safetensors\n","-rw-r--r-- 1 root root  79M Dec 31 14:57 style_encoder.safetensors\n","-rw-r--r-- 1 root root 301M Dec 31 14:57 unet.safetensors\n","drwxr-xr-x 3 root root 4.0K Dec 31 14:57 .\n","drwxr-xr-x 1 root root 4.0K Dec 31 16:28 ..\n","Attempting to load secret 'HF_TOKEN' from 'colab' environment...\n","âœ… Successfully loaded secret 'HF_TOKEN'.\n"]}],"execution_count":9},{"id":"92cff682","cell_type":"code","source":"# %cd {OUTPUT_PATH}\n# # ==========================================\n# # EXPORT / DOWNLOAD DATASET COMMANDS\n# # ==========================================\n\n# # Train Split\n# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n#   --output_dir \"my_dataset/train\" \\\n#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n#   --split \"train\" \\\n#   --token HF_TOKEN\n# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n#   --output_dir \"my_dataset/train_original\" \\\n#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n#   --split \"train_original\" \\\n#   --token HF_TOKEN\n# # Validation: Unseen Both\n# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n#   --output_dir \"my_dataset/val_unseen_both\" \\\n#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n#   --split \"val_unseen_both\" \\\n#   --token HF_TOKEN\n# # Validation: Seen Style, Unseen Char\n# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n#   --output_dir \"my_dataset/val_seen_style_unseen_char\" \\\n#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n#   --split \"val_seen_style_unseen_char\" \\\n#   --token HF_TOKEN\n# # Validation: Unseen Style, Seen Char\n# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n#   --output_dir \"my_dataset/val_unseen_style_seen_char\" \\\n#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n#   --split \"val_unseen_style_seen_char\" \\\n#   --token HF_TOKEN\n# !python FontDiffusion/export_hf_dataset_to_disk.py \\\n#   --output_dir \"my_dataset/test\" \\\n#   --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n#   --split \"test\" \\\n#   --token HF_TOKEN\n# print(\"SUCCESSFULLY EXPORT HF DATASET TO LOCAL DIRECTORY\")","metadata":{"id":"92cff682","papermill":{"duration":0.104394,"end_time":"2025-12-30T18:54:50.869230","exception":false,"start_time":"2025-12-30T18:54:50.764836","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"id":"29deed1d","cell_type":"code","source":"if is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\"\n%cd {OUTPUT_PATH}\n!python FontDiffusion/sample_batch.py \\\n    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n    --style_images \"FontDiffusion/styles_images\" \\\n    --ckpt_dir \"ckpt/\" \\\n    --ttf_path \"FontDiffusion/fonts\" \\\n    --output_dir \"my_dataset/train_original\" \\\n    --num_inference_steps 20 \\\n    --guidance_scale 7.5 \\\n    --start_line 600 \\\n    --end_line 1000 \\\n    --batch_size 24 \\\n    --save_interval 1 \\\n    --channels_last \\\n    --seed 42 \\\n    --compile \\\n    --enable_xformers","metadata":{"execution":{"iopub.status.busy":"2025-12-31T12:29:39.592177Z","iopub.execute_input":"2025-12-31T12:29:39.592560Z","iopub.status.idle":"2025-12-31T12:40:46.036972Z","shell.execute_reply.started":"2025-12-31T12:29:39.592524Z","shell.execute_reply":"2025-12-31T12:40:46.036057Z"},"id":"29deed1d","outputId":"749b50d0-75e3-4d36-e509-919188feb64c","papermill":{"duration":10.53661,"end_time":"2025-12-30T18:55:01.421093","exception":false,"start_time":"2025-12-30T18:54:50.884483","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n","\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mhuggingface-hub==0.36.0                                                       \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mfilelock==3.20.1                                                              \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mfsspec==2025.12.0                                                             \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpackaging==25.0                                                               \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mpyyaml==6.0.3                                                                 \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mrequests==2.32.5                                                              \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mtqdm==4.67.1                                                                  \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mhf-xet==1.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mhf-xet==1.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mcharset-normalizer==3.4.4                                                     \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2midna==3.11                                                                    \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2murllib3==2.6.2                                                                \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mcertifi==2025.11.12                                                           \u001b[0m\r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 63ms\u001b[0m\u001b[0m\n","\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37mâ ‹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)                                                   \r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)                                                   \r\u001b[2K\u001b[37mâ ™\u001b[0m \u001b[2m\u001b[0m (2/2)                                                                        \r\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 0.28ms\u001b[0m\u001b[0m\n","\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n","â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/2] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2Kâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [0/2] \u001b[2mfsspec==2025.12.0                                    \u001b[0m\r\u001b[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/2] \u001b[2mfsspec==2025.12.0                                    \u001b[0m\r\u001b[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ [1/2] \u001b[2mhuggingface-hub==0.36.0                              \u001b[0m\r\u001b[2Kâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ [2/2] \u001b[2mhuggingface-hub==0.36.0                              \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.12.0\u001b[0m\n"," \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n"," \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n","/content\n","/content/FontDiffusion/src/dpm_solver/dpm_solver_pytorch.py:53: SyntaxWarning: invalid escape sequence '\\h'\n","  The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n","pygame 2.6.1 (SDL 2.28.4, Python 3.12.12)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n","\n","============================================================\n","FONTDIFFUSER SYNTHESIS DATA GENERATION MAGIC\n","============================================================\n","ðŸ“– Loading characters from file: FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\n","   Lines 600 to 1000 (total file: 10174 lines)\n","   Processing 401 lines...\n","ðŸ“– Reading character file: 100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 401/401 [00:00<00:00, 2016685.74line/s]\u001b[0m\n","âœ… Successfully loaded 401 single characters.\n","\n","ðŸ“‚ Loading 15 style images from directory...\n","âœ“ Verifying style images: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 15/15 [00:00]\u001b[0m\n","\n","Initializing font manager...\n","\n","============================================================\n","Loading 15 fonts from directory...\n","============================================================\n","error: XDG_RUNTIME_DIR not set in the environment.\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HAN NOM A\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HAN NOM B\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: Han-Nom Kai 1.00\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: Han-Nom-Khai-Regular-300623\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: Han-nom Minh 1.42\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinA\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinA\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinB\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinB\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: HanaMinC\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTong-Regular\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTong-Regular\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTong-Regular2\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTongLight\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","âœ“ Loaded: NomNaTongLight2\n","============================================================\n","Successfully loaded 12 fonts\n","\n","\n","ðŸ“Š Configuration:\n","  Dataset split: train_original\n","  Characters: 401 (lines 600-1000)\n","  Styles: 15\n","  Output Directory: my_dataset/train_original\n","  Checkpoint Directory: ckpt/\n","  Device: cuda\n","  Batch Size: 24\n","Will look for results checkpoint at my_dataset/train_original/results_checkpoint.json\n","âœ“ Loaded checkpoint: 1575 unique generations\n","  Total raw entries: 1575\n","\n","Loading FontDiffuser pipeline...\n","Loading FontDiffuser pipeline...\n","Load the down block  DownBlock2D\n","Load the down block  MCADownBlock2D\n","The style_attention cross attention dim in Down Block 1 layer is 1024\n","The style_attention cross attention dim in Down Block 2 layer is 1024\n","Load the down block  MCADownBlock2D\n","The style_attention cross attention dim in Down Block 1 layer is 1024\n","The style_attention cross attention dim in Down Block 2 layer is 1024\n","Load the down block  DownBlock2D\n","Load the up block  UpBlock2D\n","Load the up block  StyleRSIUpBlock2D\n","Load the up block  StyleRSIUpBlock2D\n","Load the up block  UpBlock2D\n","Param count for Ds initialized parameters: 20591296\n","Get CG-GAN Style Encoder!\n","Param count for Ds initialized parameters: 1187008\n","Get CG-GAN Content Encoder!\n","âœ“ Loaded model state_dict successfully\n","Converting to channels-last memory format...\n","2025-12-31 16:28:57.118331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1767198537.139395   28741 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1767198537.145569   28741 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1767198537.161455   28741 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767198537.161482   28741 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767198537.161486   28741 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1767198537.161492   28741 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-12-31 16:28:57.166331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","âœ“ Model moved to device\n","âœ“ Loaded training DDPM scheduler successfully\n","âœ“ Loaded DPM-Solver pipeline successfully\n","ðŸ”§ Compiling model components with torch.compile...\n","/content/FontDiffusion/sample_batch.py:1505: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n","  if hasattr(pipe.model, \"unet\"):\n","/content/FontDiffusion/sample_batch.py:1506: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n","  pipe.model.unet = torch.compile(pipe.model.unet)\n","/content/FontDiffusion/sample_batch.py:1507: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n","  if hasattr(pipe.model, \"style_encoder\"):\n","/content/FontDiffusion/sample_batch.py:1508: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n","  pipe.model.style_encoder = torch.compile(pipe.model.style_encoder)\n","/content/FontDiffusion/sample_batch.py:1509: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n","  if hasattr(pipe.model, \"content_encoder\"):\n","/content/FontDiffusion/sample_batch.py:1511: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n","  pipe.model.content_encoder\n","âœ“ Compilation complete.\n","Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n","\n","======================================================================\n","                      GENERATING CONTENT IMAGES                       \n","======================================================================\n","\n","============================================================\n","Generating Content Images\n","Using 12 fonts\n","Characters: 401\n","============================================================\n","ðŸ“¸ Generating content images: 100% 401/401 [00:59<00:00,  6.70char/s]\n","âœ“ Generated 401 content images\n","============================================================\n","\n","======================================================================\n","                        BATCH IMAGE GENERATION                        \n","======================================================================\n","Fonts:                12\n","Styles:               15\n","Characters:           401\n","Batch size:           24\n","Existing generations: 1575 unique pairs\n","Existing hashes:      1575\n","======================================================================\n","\n","Using font: HAN NOM A\n","======================================================================\n","\n","  ðŸ”„ ref_1: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:   0% 0/15 [00:00<?, ?it/s]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A/content/FontDiffusion/src/model.py:99: FutureWarning: Accessing config attribute `style_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'style_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.style_encoder'.\n","  style_img_feature, _, style_residual_features = self.style_encoder(style_images)\n","/content/FontDiffusion/src/model.py:107: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n","  content_img_feture, content_residual_features = self.content_encoder(\n","/content/FontDiffusion/src/model.py:112: FutureWarning: Accessing config attribute `content_encoder` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'content_encoder' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.content_encoder'.\n","  style_content_feature, style_content_res_features = self.content_encoder(\n","/content/FontDiffusion/src/model.py:124: FutureWarning: Accessing config attribute `unet` directly via 'FontDiffuserModelDPM' object attribute is deprecated. Please access 'unet' over 'FontDiffuserModelDPM's config object instead, e.g. 'unet.config.unet'.\n","  out = self.unet(\n","\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:36<00:36]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:02<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_1: 40 images in 62.53s\n","ðŸŽ¨ Generating styles:   0% 0/15 [01:21<?, ?it/s]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           1/15 styles\n","Generated:          40 pairs\n","Skipped:            0 pairs\n","Elapsed time:       1.4 minutes\n","Est. remaining:     19.0 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1615 generations)\n","  ðŸ”„ ref_2: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:   7% 1/15 [01:21<18:59, 81.38s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_2: 40 images in 65.97s\n","ðŸŽ¨ Generating styles:   7% 1/15 [02:45<18:59, 81.38s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           2/15 styles\n","Generated:          80 pairs\n","Skipped:            0 pairs\n","Elapsed time:       2.8 minutes\n","Est. remaining:     17.9 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1655 generations)\n","  ðŸ”„ ref_3: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  13% 2/15 [02:45<17:59, 83.06s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_3: 40 images in 65.95s\n","ðŸŽ¨ Generating styles:  13% 2/15 [04:09<17:59, 83.06s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           3/15 styles\n","Generated:          120 pairs\n","Skipped:            0 pairs\n","Elapsed time:       4.2 minutes\n","Est. remaining:     16.6 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1695 generations)\n","  ðŸ”„ ref_4: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  20% 3/15 [04:09<16:42, 83.52s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_4: 40 images in 65.96s\n","ðŸŽ¨ Generating styles:  20% 3/15 [05:33<16:42, 83.52s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           4/15 styles\n","Generated:          160 pairs\n","Skipped:            0 pairs\n","Elapsed time:       5.6 minutes\n","Est. remaining:     15.3 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1735 generations)\n","  ðŸ”„ ref_7: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  27% 4/15 [05:33<15:20, 83.67s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_7: 40 images in 65.95s\n","ðŸŽ¨ Generating styles:  27% 4/15 [06:57<15:20, 83.67s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           5/15 styles\n","Generated:          200 pairs\n","Skipped:            0 pairs\n","Elapsed time:       7.0 minutes\n","Est. remaining:     13.9 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1775 generations)\n","  ðŸ”„ ref_8: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  33% 5/15 [06:57<13:57, 83.78s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_8: 40 images in 65.99s\n","ðŸŽ¨ Generating styles:  33% 5/15 [08:21<13:57, 83.78s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           6/15 styles\n","Generated:          240 pairs\n","Skipped:            0 pairs\n","Elapsed time:       8.4 minutes\n","Est. remaining:     12.5 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1815 generations)\n","  ðŸ”„ ref_hanh: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  40% 6/15 [08:21<12:34, 83.87s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_hanh: 40 images in 65.97s\n","ðŸŽ¨ Generating styles:  40% 6/15 [09:45<12:34, 83.87s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           7/15 styles\n","Generated:          280 pairs\n","Skipped:            0 pairs\n","Elapsed time:       9.8 minutes\n","Est. remaining:     11.2 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1855 generations)\n","  ðŸ”„ ref_hanhthu_1: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  47% 7/15 [09:45<11:11, 83.89s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:06<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_hanhthu_1: 40 images in 66.02s\n","ðŸŽ¨ Generating styles:  47% 7/15 [11:09<11:11, 83.89s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           8/15 styles\n","Generated:          320 pairs\n","Skipped:            0 pairs\n","Elapsed time:       11.2 minutes\n","Est. remaining:     9.8 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1895 generations)\n","  ðŸ”„ ref_hanhthu_2: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  53% 8/15 [11:09<09:47, 83.92s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:06<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_hanhthu_2: 40 images in 66.02s\n","ðŸŽ¨ Generating styles:  53% 8/15 [12:33<09:47, 83.92s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           9/15 styles\n","Generated:          360 pairs\n","Skipped:            0 pairs\n","Elapsed time:       12.6 minutes\n","Est. remaining:     8.4 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1935 generations)\n","  ðŸ”„ ref_khai: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  60% 9/15 [12:33<08:23, 83.96s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_khai: 40 images in 65.92s\n","ðŸŽ¨ Generating styles:  60% 9/15 [13:57<08:23, 83.96s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           10/15 styles\n","Generated:          400 pairs\n","Skipped:            0 pairs\n","Elapsed time:       14.0 minutes\n","Est. remaining:     7.0 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (1975 generations)\n","  ðŸ”„ ref_le: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  67% 10/15 [13:57<07:00, 84.02s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_le: 40 images in 65.95s\n","ðŸŽ¨ Generating styles:  67% 10/15 [15:21<07:00, 84.02s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           11/15 styles\n","Generated:          440 pairs\n","Skipped:            0 pairs\n","Elapsed time:       15.4 minutes\n","Est. remaining:     5.6 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (2015 generations)\n","  ðŸ”„ ref_lethu_1: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  73% 11/15 [15:21<05:36, 84.02s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:05<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_lethu_1: 40 images in 66.00s\n","ðŸŽ¨ Generating styles:  73% 11/15 [16:45<05:36, 84.02s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           12/15 styles\n","Generated:          480 pairs\n","Skipped:            0 pairs\n","Elapsed time:       16.8 minutes\n","Est. remaining:     4.2 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (2055 generations)\n","  ðŸ”„ ref_lethu_2: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  80% 12/15 [16:45<04:12, 84.03s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:06<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_lethu_2: 40 images in 66.11s\n","ðŸŽ¨ Generating styles:  80% 12/15 [18:09<04:12, 84.03s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           13/15 styles\n","Generated:          520 pairs\n","Skipped:            0 pairs\n","Elapsed time:       18.2 minutes\n","Est. remaining:     2.8 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (2095 generations)\n","  ðŸ”„ ref_thao: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  87% 13/15 [18:09<02:48, 84.07s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:06<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_thao: 40 images in 66.11s\n","ðŸŽ¨ Generating styles:  87% 13/15 [19:33<02:48, 84.07s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           14/15 styles\n","Generated:          560 pairs\n","Skipped:            0 pairs\n","Elapsed time:       19.6 minutes\n","Est. remaining:     1.4 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (2135 generations)\n","  ðŸ”„ ref_trien: Generating 401/401 new images\n","ðŸŽ¨ Generating styles:  93% 14/15 [19:34<01:24, 84.09s/it]\n","    ðŸ“¸ Preparing HAN NOM A: 0/40 |\u001b[36m                                                         \u001b[0m| [00:00]\u001b[0m\u001b[A\n","                                                                                                    \u001b[A\n","\n","    ðŸŽ¨ Inferencing: 0/2 |\u001b[33m                                                                \u001b[0m| [00:00<?]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 1/2 |\u001b[33mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              \u001b[0m| [00:39<00:39]\u001b[0m\u001b[A\u001b[A\n","\n","    ðŸŽ¨ Inferencing: 3/? |\u001b[33m                                                            \u001b[0m| [01:06<00:00]\u001b[0m\u001b[A\u001b[A\n","\n","  âœ“ ref_trien: 40 images in 66.09s\n","ðŸŽ¨ Generating styles:  93% 14/15 [20:58<01:24, 84.09s/it]\n","======================================================================\n","                              CHECKPOINT                              \n","======================================================================\n","Progress:           15/15 styles\n","Generated:          600 pairs\n","Skipped:            0 pairs\n","Elapsed time:       21.0 minutes\n","Est. remaining:     0.0 minutes\n","======================================================================\n","  âœ… Saved results_checkpoint.json (2175 generations)\n","ðŸŽ¨ Generating styles: 100% 15/15 [20:58<00:00, 83.88s/it]\n","\n","======================================================================\n","                         GENERATION COMPLETE                          \n","======================================================================\n","\n","Pair Statistics:\n","  Total possible:     6015\n","  Generated (new):    600\n","  Skipped (exist):    0\n","  Failed (no font):   0\n","\n","Timing:\n","  Total time:         21.0 minutes (1258s)\n","  Avg per pair:       2097.0ms\n","======================================================================\n","\n","ðŸ’¾ Saving final checkpoint...\n","  âœ… Saved results_checkpoint.json (2175 generations)\n","\n","============================================================\n","âœ… GENERATION COMPLETE!\n","============================================================\n","\n","Output structure:\n","  my_dataset/train_original/\n","    â”œâ”€â”€ ContentImage/\n","    â”‚   â”œâ”€â”€ U+XXXX_char_hash.png\n","    â”‚   â””â”€â”€ ...\n","    â”œâ”€â”€ TargetImage/\n","    â”‚   â”œâ”€â”€ style0/\n","    â”‚   â”‚   â”œâ”€â”€ U+XXXX_char_style0_hash.png\n","    â”‚   â”‚   â””â”€â”€ ...\n","    â”‚   â””â”€â”€ ...\n","    â””â”€â”€ results_checkpoint.json âœ… (single source of truth)\n"]}],"execution_count":11},{"id":"f9250a14","cell_type":"code","source":"!python FontDiffusion/create_validation_split.py \\\n  --data_root my_dataset \\\n  --val_ratio 0.2 \\\n  --seed 42","metadata":{"execution":{"iopub.status.busy":"2025-12-31T12:41:13.490300Z","iopub.execute_input":"2025-12-31T12:41:13.491082Z","iopub.status.idle":"2025-12-31T12:41:14.952535Z","shell.execute_reply.started":"2025-12-31T12:41:13.491048Z","shell.execute_reply":"2025-12-31T12:41:14.951858Z"},"id":"f9250a14","outputId":"0f834d09-da00-4aa4-f486-6e70981b4137","papermill":{"duration":0.236541,"end_time":"2025-12-30T18:55:01.673705","exception":false,"start_time":"2025-12-30T18:55:01.437164","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","FONTDIFFUSION SPLIT CREATOR (SIMPLIFIED)\n","============================================================\n","âœ“ Using source directory: my_dataset/train_original\n","\n","============================================================\n","CREATING DATA SPLITS\n","============================================================\n","\n","============================================================\n","ANALYZING TRAINING DATA\n","============================================================\n","\n","ðŸ” Scanning target images...\n","\rStyles:   0%|                                                             | 0/15 [00:00<?, ?style/s]\rStyles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 1208.97style/s]\n","\n","âœ“ Initial scan:\n","  Styles: 15\n","  Characters: 141\n","  Valid (char, style) pairs: 2115\n","\n","ðŸ” Auto-detecting font parameter from existing files...\n","  ðŸ“‚ Found 15 fonts in fonts/ directory\n","  âœ“ Detected font parameter: 'HAN NOM A'\n","    Verified with: U+34E4_ã“¤_20242aa0.png\n","\n","ðŸ” Validating content images (using font='HAN NOM A')...\n","\rChecking content:   0%|                                                   | 0/141 [00:00<?, ?char/s]\rChecking content: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:00<00:00, 62114.99char/s]\n","\n","âš ï¸  WARNING: 46/141 characters missing content images:\n","  Examples: ['ð „©', 'ð ‘¬', 'ð ¤†', 'ð ¦³', 'ð ³’', 'ð ´', 'ð º¥', 'ð¡Š°', 'ð¡—‰', 'ð¡¥µ']...\n","  These characters will be excluded from splits\n","\n","âœ… After validation:\n","  Characters with content images: 95\n","  Valid (char, style) pairs: 1425\n","  Content images found: 95\n","\n","ðŸ“Š Character distribution per style:\n","  ref_1: 95 characters\n","  ref_2: 95 characters\n","  ref_3: 95 characters\n","  ref_4: 95 characters\n","  ref_7: 95 characters\n","  ref_8: 95 characters\n","  ref_hanh: 95 characters\n","  ref_hanhthu_1: 95 characters\n","  ref_hanhthu_2: 95 characters\n","  ref_khai: 95 characters\n","  ... and 5 more styles\n","\n","============================================================\n","CREATING TRAIN/VAL SPLITS\n","============================================================\n","\n","ðŸ“Š Split Statistics:\n","  Styles: 12 train + 3 val\n","  Chars:  76 train + 19 val\n","\n","ðŸ“‹ Splits:\n","\n","  train:\n","    Description: Training data (seen styles + seen characters)\n","    Styles: 12 (['ref_lethu_1', 'ref_hanhthu_1', 'ref_trien']...)\n","    Chars: 76 (['è‹', 'å ›', 'ã§…', 'å—•', 'æ‘±']...)\n","\n","  val:\n","    Description: Validation data (unseen styles + unseen characters)\n","    Styles: 3 (['ref_1', 'ref_2', 'ref_le']...)\n","    Chars: 19 (['ç‡¶', 'ã–”', 'ãº§', 'ã–¡', 'ã¹']...)\n","\n","ðŸ“ Creating train split:\n","  Valid pairs: 912\n","  Unique chars: 76\n","  Unique styles: 12\n","\n","  ðŸ“¥ Copying content images (font='HAN NOM A')...\n","  ðŸ“¥ Copying target images (font='HAN NOM A')...\n","                                                                                \n","  ðŸ” Validating split...\n","  âœ“ All 912 targets have matching content\n","\n","  ðŸ“Š Summary:\n","    Content copied: 76\n","    Target copied: 912\n","    Skipped: 0\n","  âœ“ Copied 76 content + 912 target (skipped 0)\n","\n","  ðŸ“‹ Filtering results_checkpoint.json for train...\n","    âœ“ Saved checkpoint: 960/2175 generations\n","    ðŸ“„ my_dataset/train/results_checkpoint.json\n","\n","ðŸ“ Creating val split:\n","  Valid pairs: 57\n","  Unique chars: 19\n","  Unique styles: 3\n","\n","  ðŸ“¥ Copying content images (font='HAN NOM A')...\n","  ðŸ“¥ Copying target images (font='HAN NOM A')...\n","                                                                                \n","  ðŸ” Validating split...\n","  âœ“ All 57 targets have matching content\n","\n","  ðŸ“Š Summary:\n","    Content copied: 19\n","    Target copied: 57\n","    Skipped: 0\n","  âœ“ Copied 19 content + 57 target (skipped 0)\n","\n","  ðŸ“‹ Filtering results_checkpoint.json for val...\n","    âœ“ Saved checkpoint: 57/2175 generations\n","    ðŸ“„ my_dataset/val/results_checkpoint.json\n","\n","âœ“ Saved split metadata to my_dataset/split_info.json\n","\n","============================================================\n","âœ“ SPLIT CREATION COMPLETE\n","============================================================\n","\n","âœ… Created directories:\n","  ðŸ“ train/ - Training data (seen styles + seen chars)\n","    â”œâ”€â”€ ContentImage/\n","    â”œâ”€â”€ TargetImage/\n","    â””â”€â”€ results_checkpoint.json\n","  ðŸ“ val/ - Validation data (unseen styles + unseen chars)\n","    â”œâ”€â”€ ContentImage/\n","    â”œâ”€â”€ TargetImage/\n","    â””â”€â”€ results_checkpoint.json\n","\n","ðŸ’¡ Each folder guarantees:\n","  âœ“ Every target image has matching content image\n","  âœ“ Parsed by codepoint only (no reliance on safe char)\n","  âœ“ Filtered results_checkpoint.json with split-specific data\n"]}],"execution_count":22},{"id":"79508d80-fac1-4318-9174-a32613a557e3","cell_type":"code","source":"!uv pip install --upgrade pyarrow datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T12:43:11.967796Z","iopub.execute_input":"2025-12-31T12:43:11.968477Z","iopub.status.idle":"2025-12-31T12:43:22.493086Z","shell.execute_reply.started":"2025-12-31T12:43:11.968416Z","shell.execute_reply":"2025-12-31T12:43:22.492187Z"},"id":"79508d80-fac1-4318-9174-a32613a557e3"},"outputs":[],"execution_count":null},{"id":"e92a9392","cell_type":"code","source":"# # --- RAW DATA (Before Splitting) ---\n# !python FontDiffusion/create_hf_dataset.py \\\n#   --data_dir \"my_dataset/train_original\" \\\n#   --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n#   --split \"train_original\" \\\n#   --private \\\n#   --token \"{HF_TOKEN}\"\n\n# # --- ORGANIZED SPLITS (After Splitting) ---\n\n# # Train Split\n# !python FontDiffusion/create_hf_dataset.py \\\n#   --data_dir \"my_dataset/train\" \\\n#   --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n#   --split \"train\" \\\n#   --private \\\n#   --token \"{HF_TOKEN}\"\n\n# # Test Split\n# !python FontDiffusion/create_hf_dataset.py \\\n#   --data_dir \"my_dataset/test\" \\\n#   --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n#   --split \"test\" \\\n#   --private \\\n#   --token \"{HF_TOKEN}\"\n\n# # Validation: Unseen Both\n# !python FontDiffusion/create_hf_dataset.py \\\n#   --data_dir \"my_dataset/val_unseen_both\" \\\n#   --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n#   --split \"val_unseen_both\" \\\n#   --private \\\n#   --token \"{HF_TOKEN}\"\n\n# # Validation: Seen Style, Unseen Char\n# !python FontDiffusion/create_hf_dataset.py \\\n#   --data_dir \"my_dataset/val_seen_style_unseen_char\" \\\n#   --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n#   --split \"val_seen_style_unseen_char\" \\\n#   --private \\\n#   --token \"{HF_TOKEN}\"\n\n# # Validation: Unseen Style, Seen Char\n# !python FontDiffusion/create_hf_dataset.py \\\n#   --data_dir \"my_dataset/val_unseen_style_seen_char\" \\\n#   --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n#   --split \"val_unseen_style_seen_char\" \\\n#   --private \\\n#   --token \"{HF_TOKEN}\"\n# print(\"SUCCESSFULLY UPLOAD LOCAL MY_DATASET TO HUGGINGFACE DATASETS SPACE\")","metadata":{"execution":{"iopub.status.busy":"2025-12-31T12:43:24.896976Z","iopub.execute_input":"2025-12-31T12:43:24.897792Z","iopub.status.idle":"2025-12-31T12:45:02.938686Z","shell.execute_reply.started":"2025-12-31T12:43:24.897742Z","shell.execute_reply":"2025-12-31T12:45:02.937689Z"},"id":"e92a9392","papermill":{"duration":21.070659,"end_time":"2025-12-30T18:55:22.760587","exception":false,"start_time":"2025-12-30T18:55:01.689928","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"vRL8QovYCvLY","cell_type":"code","source":"!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train_original\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"train\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val\" \\\n  --repo_id \"{HF_USERNAME}/font-diffusion-generated-data\" \\\n  --split \"val\" \\\n  --private \\\n  --token \"{HF_TOKEN}\"\n","metadata":{"id":"vRL8QovYCvLY","outputId":"08301c52-4ae1-4268-c516-2ff8bd834783","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","\n","Data dir: my_dataset/train_original\n","Repo: dzungpham/font-diffusion-generated-data\n","Push to Hub: True\n","âœ“ Validated directory structure\n","  Content images: my_dataset/train_original/ContentImage\n","  Target images: my_dataset/train_original/TargetImage\n","  Results checkpoint: my_dataset/train_original/results_checkpoint.json\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","âœ“ Loaded results_checkpoint.json\n","  Generations: 2175\n","  Characters: 401\n","  Styles: 15\n","\n","ðŸ–¼ï¸  Loading 2175 image pairs...\n","Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2175/2175 [00:01<00:00, 1242.16pair/s]\n","âœ“ Loaded 2175 samples\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: train_original\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 2175/2175 [00:00<00:00, 28905.61 examples/s]\n","\n","Creating parquet from Arrow format: 100% 1/1 [00:00<00:00, 26.14ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              : 100% 18.5M/18.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 18.5M/18.5M [00:00<00:00, 40.0MB/s,   ???B/s  ]\n","\n","                              : 100% 18.5M/18.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","                              : 100% 18.5M/18.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 18.5M/18.5M [00:00<00:00, 21.4MB/s,  0.00B/s  ]\n","New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n","                              : 100% 18.5M/18.5M [00:00<?, ?B/s]\n","Uploading the dataset shards: 100% 1/1 [00:01<00:00,  1.33s/ shards]\n","No files have been modified since last commit. Skipping to prevent empty commit.\n","\n","âœ“ Successfully pushed to Hub!\n","  URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ… COMPLETE!\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","\n","Data dir: my_dataset/train\n","Repo: dzungpham/font-diffusion-generated-data\n","Push to Hub: True\n","âœ“ Validated directory structure\n","  Content images: my_dataset/train/ContentImage\n","  Target images: my_dataset/train/TargetImage\n","  Results checkpoint: my_dataset/train/results_checkpoint.json\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","âœ“ Loaded results_checkpoint.json\n","  Generations: 960\n","  Characters: 0\n","  Styles: 0\n","\n","ðŸ–¼ï¸  Loading 960 image pairs...\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","âš  Missing content: my_dataset/train/ContentImage/U+36EA_ã›ª_2d26c8cc.png\n","âš  Missing content: my_dataset/train/ContentImage/U+3915_ã¤•_f2f01402.png\n","âš  Missing content: my_dataset/train/ContentImage/U+54B9_å’¹_0e829cc9.png\n","âš  Missing content: my_dataset/train/ContentImage/U+47DC_äŸœ_8e570fa5.png\n","Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:00<00:00, 1339.74pair/s]\n","âœ“ Loaded 912 samples\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: train\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 912/912 [00:00<00:00, 31172.22 examples/s]\n","\n","Creating parquet from Arrow format: 100% 1/1 [00:00<00:00, 57.96ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              :  10% 773k/7.70M [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (0 / 1)      :  10% 773k/7.70M [00:00<00:04, 1.51MB/s, 3.87MB/s  ]\n","New Data Upload               :   7% 524k/7.46M [00:00<00:06, 1.03MB/s, 2.62MB/s  ]\u001b[A\n","\n","Processing Files (0 / 1)      :  17% 1.30M/7.70M [00:00<00:03, 1.91MB/s, 3.24MB/s  ]\n","New Data Upload               :  14% 1.05M/7.46M [00:00<00:04, 1.60MB/s, 2.62MB/s  ]\u001b[A\n","\n","Processing Files (0 / 1)      :  71% 5.49M/7.70M [00:00<00:00, 8.37MB/s, 9.15MB/s  ]\n","New Data Upload               :  70% 5.24M/7.46M [00:00<00:00, 8.16MB/s, 8.73MB/s  ]\u001b[A\n","\n","Processing Files (0 / 1)      :  99% 7.59M/7.70M [00:01<00:00, 9.07MB/s, 9.49MB/s  ]\n","New Data Upload               :  98% 7.34M/7.46M [00:01<00:00, 8.93MB/s, 9.18MB/s  ]\u001b[A\n","\n","                              :  99% 7.59M/7.70M [00:00<00:00, 8.52MB/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 7.70M/7.70M [00:01<00:00, 4.83MB/s, 6.42MB/s  ]\n","New Data Upload               : 100% 7.46M/7.46M [00:01<00:00, 4.75MB/s, 6.21MB/s  ]\u001b[A\n","\n","                              : 100% 7.70M/7.70M [00:01<00:00, 6.63MB/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 7.70M/7.70M [00:01<00:00, 4.50MB/s, 5.50MB/s  ]\n","New Data Upload               : 100% 7.46M/7.46M [00:01<00:00, 4.36MB/s, 5.33MB/s  ]\n","                              : 100% 7.70M/7.70M [00:01<00:00, 5.78MB/s]\n","Uploading the dataset shards: 100% 1/1 [00:02<00:00,  2.07s/ shards]\n","\n","âœ“ Successfully pushed to Hub!\n","  URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ… COMPLETE!\n","\n","============================================================\n","FONTDIFFUSION DATASET CREATOR\n","============================================================\n","\n","Data dir: my_dataset/val\n","Repo: dzungpham/font-diffusion-generated-data\n","Push to Hub: True\n","âœ“ Validated directory structure\n","  Content images: my_dataset/val/ContentImage\n","  Target images: my_dataset/val/TargetImage\n","  Results checkpoint: my_dataset/val/results_checkpoint.json\n","\n","============================================================\n","BUILDING DATASET\n","============================================================\n","\n","âœ“ Loaded results_checkpoint.json\n","  Generations: 57\n","  Characters: 0\n","  Styles: 0\n","\n","ðŸ–¼ï¸  Loading 57 image pairs...\n","Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:00<00:00, 810.94pair/s]\n","âœ“ Loaded 57 samples\n","\n","============================================================\n","PUSHING TO HUB\n","============================================================\n","Repository: dzungpham/font-diffusion-generated-data\n","Split: val\n","Uploading the dataset shards:   0% 0/1 [00:00<?, ? shards/s]\n","Map: 100% 57/57 [00:00<00:00, 7802.72 examples/s]\n","\n","Creating parquet from Arrow format: 100% 1/1 [00:00<00:00, 357.97ba/s]\n","Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n","New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n","\n","                              :  93% 526k/563k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (0 / 1)      :  93% 526k/563k [00:00<00:00, 1.23MB/s, 2.62MB/s  ]\n","New Data Upload               :  93% 526k/563k [00:00<00:00, 1.23MB/s, 2.62MB/s  ]\u001b[A\n","\n","                              :  93% 526k/563k [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 563k/563k [00:00<00:00, 579kB/s,  937kB/s  ] \n","New Data Upload               : 100% 563k/563k [00:00<00:00, 579kB/s,  937kB/s  ] \u001b[A\n","\n","                              : 100% 563k/563k [00:00<00:00, 79.7kB/s]\u001b[A\u001b[A\n","\n","Processing Files (1 / 1)      : 100% 563k/563k [00:01<00:00, 547kB/s,  703kB/s  ]\n","New Data Upload               : 100% 563k/563k [00:01<00:00, 547kB/s,  703kB/s  ]\n","                              : 100% 563k/563k [00:00<00:00, 61.9kB/s]\n","Uploading the dataset shards: 100% 1/1 [00:01<00:00,  1.39s/ shards]\n","README.md: 100% 640/640 [00:00<00:00, 5.66MB/s]\n","\n","âœ“ Successfully pushed to Hub!\n","  URL: https://huggingface.co/datasets/dzungpham/font-diffusion-generated-data\n","\n","âœ… COMPLETE!\n"]}],"execution_count":23},{"id":"a87caab2","cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"a87caab2","papermill":{"duration":1.992585,"end_time":"2025-12-30T18:55:24.769269","exception":false,"start_time":"2025-12-30T18:55:22.776684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"267634e8","cell_type":"code","source":"# TRAINING PHASE 1\n!uv pip install \"huggingface-hub==0.25.2\"\nimport wandb\n\n# Load your Weights & Biases API key from a secure store\nwandb_key = load_secret(\"WANDB_API_KEY\")\nwandb.login(key=wandb_key)\n\n# Run the training script with the corrected flag syntax\n!accelerate launch FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=FontDiffuser_training_phase_1 \\\n    --data_root=my_dataset \\\n    --output_dir=outputs/FontDiffuser \\\n    --report_to=wandb \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.7 \\\n    --max_train_steps=200 \\\n    --ckpt_interval=100 \\\n    --val_interval=200 \\\n    --gradient_accumulation_steps=1 \\\n    --log_interval=50 \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler=linear \\\n    --lr_warmup_steps=10000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=no","metadata":{"execution":{"iopub.status.busy":"2025-12-31T12:48:49.468622Z","iopub.execute_input":"2025-12-31T12:48:49.468916Z","iopub.status.idle":"2025-12-31T12:49:05.737480Z","shell.execute_reply.started":"2025-12-31T12:48:49.468888Z","shell.execute_reply":"2025-12-31T12:49:05.736719Z"},"id":"267634e8","papermill":{"duration":0.021927,"end_time":"2025-12-30T18:55:24.807644","exception":false,"start_time":"2025-12-30T18:55:24.785717","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f","cell_type":"code","source":"!ls -lr outputs/FontDiffuser","metadata":{"trusted":true,"id":"cb2a3326-6bfa-4fba-bbcc-e2e7be996c7f"},"outputs":[],"execution_count":null},{"id":"97f8136e","cell_type":"code","source":"# TRAINING PHASE 2\n!wandb login\n!python FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_2\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --phase_2 \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_2000\" \\\n    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n    --sc_coefficient=0.05 \\\n    --num_neg=13 \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.4 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-5 \\\n    --lr_scheduler=\"constant\" \\\n    --lr_warmup_steps=1000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n","metadata":{"id":"97f8136e","papermill":{"duration":0.022471,"end_time":"2025-12-30T18:55:24.845778","exception":false,"start_time":"2025-12-30T18:55:24.823307","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"88c45e2f","cell_type":"code","source":"!python FontDiffusion/pth2safetensors.py \\\n    --weights_dir \"ckpt\" \\\n    --repo_id \"dzungpham/font-diffusion-weights\" \\\n    --token \"{HF_TOKEN}\"","metadata":{"id":"88c45e2f","papermill":{"duration":0.217876,"end_time":"2025-12-30T18:55:25.079820","exception":false,"start_time":"2025-12-30T18:55:24.861944","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"5868b20b","cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\nfrom typing import List\ndef find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n\ndef zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n    folder_name = folder_path.name\n    zip_path = output_base_path / f\"{folder_name}.zip\"\n    try:\n        print(f\"   -> Zipping folder: {folder_name}...\")\n        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in folder_path.rglob(\"*\"):\n                if file_path.is_file():\n                    arcname = file_path.relative_to(folder_path.parent)\n                    zipf.write(file_path, arcname)\n        print(f\"   âœ… Created ZIP: {zip_path.name}\")\n        return True\n    except Exception as exc:\n        print(f\"   âŒ Failed to zip {folder_name}: {exc}\")\n        return False\n\ndef zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n    base = Path(output_base_path)\n    base.mkdir(parents=True, exist_ok=True)\n    result_folders = find_result_folders(base, pattern_name)\n    if not result_folders:\n        print(f\"âš ï¸ No folders matching '*dataset' found in '{output_base_path}'.\")\n        return\n    print(f\"ðŸ” Found {len(result_folders)} result folder(s) to zip.\")\n    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n    print(f\"\\nâœ… DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n\nif __name__ == \"__main__\":\n    try:\n        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n        if not output_root:\n            raise ValueError(\"OUTPUT_PATH not defined\")\n        zip_stats_results_folders(\n            output_base_path=OUTPUT_PATH,\n            pattern_name=\"my_dataset\")\n    except Exception as e:\n        print(f\"âŒ An error occurred: {e}\")","metadata":{"id":"5868b20b","papermill":{"duration":0.031197,"end_time":"2025-12-30T18:55:25.126961","exception":false,"start_time":"2025-12-30T18:55:25.095764","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}