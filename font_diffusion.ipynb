{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWFvN9XJxf9K",
        "outputId": "209a0c79-40fc-4ce5-f373-c37ac34b37af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m116 packages\u001b[0m \u001b[2min 72ms\u001b[0m\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `tokenizers==0.13.3`\n",
            "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
            "\u001b[31m      \u001b[0mrunning bdist_wheel\n",
            "\u001b[31m      \u001b[0mrunning build\n",
            "\u001b[31m      \u001b[0mrunning build_py\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mrunning build_ext\n",
            "\u001b[31m      \u001b[0mrunning build_rust\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
            "\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpDRPTkW/lib/python3.12/site-packages/setuptools/dist.py:759:\n",
            "\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n",
            "\u001b[31m      \u001b[0mSPDX license expression:\n",
            "\n",
            "\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n",
            "\n",
            "\u001b[31m      \u001b[0m        See\n",
            "\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n",
            "\u001b[31m      \u001b[0mfor details.\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\u001b[31m      \u001b[0m  self._finalize_license_expression()\n",
            "\u001b[31m      \u001b[0merror: can't find Rust compiler\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n",
            "\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n",
            "\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "\n",
            "\u001b[31m      \u001b[0mTo update pip, run:\n",
            "\n",
            "\u001b[31m      \u001b[0m    pip install --upgrade pip\n",
            "\n",
            "\u001b[31m      \u001b[0mand then retry package installation.\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n",
            "\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n",
            "\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n",
            "\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n",
            "\u001b[31m      \u001b[0mRust compiler toolchain.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
            "\u001b[31m      \u001b[0menvironment.\n",
            "\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n",
            "        depends on `\u001b[36mtokenizers\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 117ms\u001b[0m\u001b[0m\n",
            "\n",
            "‚¨áÔ∏è Installing PyTorch 1.13 (Required for this model)...\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause torch==1.13.1+cu117 has no wheels with a matching Python ABI\n",
            "\u001b[31m      \u001b[0mtag (e.g., `\u001b[36mcp312\u001b[39m`) and you require torch==1.13.1+cu117, we can conclude\n",
            "\u001b[31m      \u001b[0mthat your requirements are unsatisfiable.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m `\u001b[36mtorch\u001b[39m` was found on \u001b[36mhttps://download.pytorch.org/whl/cu117\u001b[39m, but\n",
            "\u001b[31m      \u001b[0mnot at the requested version (\u001b[36mtorch==1.13.1+cu117\u001b[39m). A compatible version\n",
            "\u001b[31m      \u001b[0mmay be available on a subsequent index (e.g., \u001b[36mhttps://pypi.org/simple\u001b[39m).\n",
            "\u001b[31m      \u001b[0mBy default, uv will only consider versions that are published on the\n",
            "\u001b[31m      \u001b[0mfirst index that contains a given package, to avoid dependency confusion\n",
            "\u001b[31m      \u001b[0mattacks. If all indexes are equally trusted, use `\u001b[32m--index-strategy\n",
            "\u001b[31m      \u001b[0munsafe-best-match\u001b[39m` to consider all versions from all indexes, regardless\n",
            "\u001b[31m      \u001b[0mof the order in which they were defined.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n",
            "\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1+cu117\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`,\n",
            "\u001b[31m      \u001b[0m`\u001b[36mcp38\u001b[39m`, `\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n",
            "\n",
            "‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n",
            "  \u001b[31m√ó\u001b[0m No solution found when resolving dependencies:\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mBecause torch==1.13.1 has no wheels with a matching Python ABI tag\n",
            "\u001b[31m      \u001b[0m(e.g., `\u001b[36mcp312\u001b[39m`) and xformers==0.0.16 depends on torch==1.13.1, we can\n",
            "\u001b[31m      \u001b[0mconclude that xformers==0.0.16 cannot be used.\n",
            "\u001b[31m      \u001b[0mAnd because you require xformers==0.0.16, we can conclude that your\n",
            "\u001b[31m      \u001b[0mrequirements are unsatisfiable.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You require \u001b[36mCPython 3.12\u001b[39m (`\u001b[36mcp312\u001b[39m`), but we only found wheels for\n",
            "\u001b[31m      \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv1.13.1\u001b[39m) with the following Python ABI tags: `\u001b[36mcp37m\u001b[39m`, `\u001b[36mcp38\u001b[39m`,\n",
            "\u001b[31m      \u001b[0m`\u001b[36mcp39\u001b[39m`, `\u001b[36mcp310\u001b[39m`, `\u001b[36mcp311\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m47 packages\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n",
            "\u001b[2K  \u001b[31m√ó\u001b[0m Failed to build `tokenizers==0.13.3`\n",
            "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
            "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta.build_wheel` failed (exit status: 1)\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stdout]\u001b[39m\n",
            "\u001b[31m      \u001b[0mrunning bdist_wheel\n",
            "\u001b[31m      \u001b[0mrunning build\n",
            "\u001b[31m      \u001b[0mrunning build_py\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/byte_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_unigram.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/char_level_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/sentencepiece_bpe.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/base_tokenizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/implementations/bert_wordpiece.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/__init__.py ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/models/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/models\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/decoders/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/normalizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/pre_tokenizers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/processors/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/trainers/__init__.pyi ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
            "\u001b[31m      \u001b[0mcopying py_src/tokenizers/tools/visualizer-styles.css ->\n",
            "\u001b[31m      \u001b[0mbuild/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
            "\u001b[31m      \u001b[0mrunning build_ext\n",
            "\u001b[31m      \u001b[0mrunning build_rust\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
            "\u001b[31m      \u001b[0m/root/.cache/uv/builds-v0/.tmpzbqTAr/lib/python3.12/site-packages/setuptools/dist.py:759:\n",
            "\u001b[31m      \u001b[0mSetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\u001b[31m      \u001b[0m        Please consider removing the following classifiers in favor of a\n",
            "\u001b[31m      \u001b[0mSPDX license expression:\n",
            "\n",
            "\u001b[31m      \u001b[0m        License :: OSI Approved :: Apache Software License\n",
            "\n",
            "\u001b[31m      \u001b[0m        See\n",
            "\u001b[31m      \u001b[0mhttps://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license\n",
            "\u001b[31m      \u001b[0mfor details.\n",
            "\u001b[31m      \u001b[0m\n",
            "\u001b[31m      \u001b[0m********************************************************************************\n",
            "\n",
            "\u001b[31m      \u001b[0m!!\n",
            "\u001b[31m      \u001b[0m  self._finalize_license_expression()\n",
            "\u001b[31m      \u001b[0merror: can't find Rust compiler\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you are using an outdated pip version, it is possible a prebuilt\n",
            "\u001b[31m      \u001b[0mwheel is available for this package but pip is not able to install from\n",
            "\u001b[31m      \u001b[0mit. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "\n",
            "\u001b[31m      \u001b[0mTo update pip, run:\n",
            "\n",
            "\u001b[31m      \u001b[0m    pip install --upgrade pip\n",
            "\n",
            "\u001b[31m      \u001b[0mand then retry package installation.\n",
            "\n",
            "\u001b[31m      \u001b[0mIf you did intend to build this package from source, try installing\n",
            "\u001b[31m      \u001b[0ma Rust compiler from your system package manager and ensure it is\n",
            "\u001b[31m      \u001b[0mon the PATH during installation. Alternatively, rustup (available at\n",
            "\u001b[31m      \u001b[0mhttps://rustup.rs) is the recommended way to download and update the\n",
            "\u001b[31m      \u001b[0mRust compiler toolchain.\n",
            "\n",
            "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
            "\u001b[31m      \u001b[0menvironment.\n",
            "\u001b[36m  help: \u001b[0m`\u001b[36mtokenizers\u001b[39m` (\u001b[36mv0.13.3\u001b[39m) was included because `\u001b[36mtransformers\u001b[39m` (\u001b[36mv4.33.1\u001b[39m)\n",
            "        depends on `\u001b[36mtokenizers\u001b[39m`\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 110ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 99ms\u001b[0m\u001b[0m\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "dos2unix is already the newest version (7.4.2-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "\n",
            "‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n"
          ]
        }
      ],
      "source": [
        "# @title Environment Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. *** FIX: Clear problematic environment variable for matplotlib ***\n",
        "# This prevents the \"ValueError: Key backend: 'module://matplotlib_inline.backend_inline'\" error\n",
        "if 'MPLBACKEND' in os.environ:\n",
        "    del os.environ['MPLBACKEND']\n",
        "    print(\"MPLBACKEND environment variable cleared.\")\n",
        "\n",
        "# 2. Clone the repository\n",
        "if not os.path.exists(\"FontDiffusion\"):\n",
        "    print(\"‚¨áÔ∏è Cloning repository...\")\n",
        "    !git clone https://github.com/dzungphieuluuky/FontDiffusion.git\n",
        "\n",
        "!uv pip install -r FontDiffusion/requirements.txt\n",
        "!uv pip install gdown\n",
        "# 3. Install PyTorch 1.13\n",
        "print(\"\\n‚¨áÔ∏è Installing PyTorch 1.13 (Required for this model)...\")\n",
        "# Force reinstall torch 1.13 to match the model's training environment\n",
        "!uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "# 4. Install other dependencies\n",
        "print(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n",
        "# Install xformers compatible with Torch 1.13\n",
        "!uv pip install xformers==0.0.16 -q\n",
        "\n",
        "# Install Transformers & Diffusers\n",
        "!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n",
        "!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n",
        "!uv pip install lpips scikit-image\n",
        "# -----------------------------------------------------------------\n",
        "!sudo apt-get update && sudo apt-get install dos2unix\n",
        "print(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxdyquWfaqdm",
        "outputId": "2997ced0-9dfd-4808-dfb6-152ee77fb722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment: Google Colab\n",
            "üìÇ Data Path: /content/\n",
            "üì¶ Output Path: /content/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "\n",
        "def configure_environment_paths():\n",
        "    \"\"\"Detect environment and configure paths\"\"\"\n",
        "    try:\n",
        "        if \"google.colab\" in str(get_ipython()):\n",
        "            print(\"‚úÖ Environment: Google Colab\")\n",
        "            base_data_path = \"/content/\"\n",
        "            base_output_path = \"/content/\"\n",
        "            environment_name = \"colab\"\n",
        "        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
        "            print(\"‚úÖ Environment: Kaggle\")\n",
        "            base_data_path = \"/kaggle/input/\"\n",
        "            base_output_path = \"/kaggle/working/\"\n",
        "            environment_name = \"kaggle\"\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n",
        "            base_data_path = \"./data/\"\n",
        "            base_output_path = \"./output/\"\n",
        "            environment_name = \"local\"\n",
        "    except NameError:\n",
        "        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n",
        "        base_data_path = \"./data/\"\n",
        "        base_output_path = \"./output/\"\n",
        "        environment_name = \"local\"\n",
        "\n",
        "    os.makedirs(base_output_path, exist_ok=True)\n",
        "    print(f\"üìÇ Data Path: {base_data_path}\")\n",
        "    print(f\"üì¶ Output Path: {base_output_path}\")\n",
        "\n",
        "    return base_data_path, base_output_path, environment_name\n",
        "\n",
        "\n",
        "INPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h06w314Jaqdm",
        "outputId": "9c909f9f-68f5-4139-e94a-5572ca78b5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "if \"colab\" in ENV_NAME:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        # Ensure 'WANDB_API_KEY' is the exact name in your Colab Secrets (the key icon)\n",
        "        wandb_key = userdata.get(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Colab Secrets: {e}\")\n",
        "\n",
        "# 2. Check if running in Kaggle\n",
        "elif \"kaggle\" in ENV_NAME:\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "        user_secrets = UserSecretsClient()\n",
        "        wandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "        wandb.login(key=wandb_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve W&B API key from Kaggle Secrets: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/12hfuZ9MQvXqcteNuz7JQ2B_mUcTr-5jZ\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PsLgUs0cYmO",
        "outputId": "d87184bf-05be-47f6-dfd3-8feafebc9a34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/ckpt/content_encoder.pth',\n",
              " '/content/ckpt/scr_210000.pth',\n",
              " '/content/ckpt/style_encoder.pth',\n",
              " '/content/ckpt/unet.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfc18e0",
        "outputId": "d9281e5e-17b9-4e35-ca80-efd436369a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping /content/ckpt.zip...\n",
            "[/content/ckpt.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/ckpt.zip or\n",
            "        /content/ckpt.zip.zip, and cannot find /content/ckpt.zip.ZIP, period.\n",
            "Unzipping of /content/ckpt.zip complete.\n"
          ]
        }
      ],
      "source": [
        "# @title Unzipping all archived files\n",
        "import os\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n",
        "\n",
        "if not zip_file_paths:\n",
        "    print(f'No .zip files found in {INPUT_PATH}.')\n",
        "else:\n",
        "    for zip_file_path in zip_file_paths:\n",
        "        if os.path.exists(zip_file_path):\n",
        "            print(f'Unzipping {zip_file_path}...')\n",
        "            !unzip -q -o {zip_file_path} -d ./\n",
        "            print(f'Unzipping of {zip_file_path} complete.')\n",
        "        else:\n",
        "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBflCTABxlF4",
        "outputId": "a894ec88-3845-40c2-bd67-b93fb7376746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ckpt\n",
            "\n",
            "‚úÖ All weights found! You can proceed to the next step.\n"
          ]
        }
      ],
      "source": [
        "# @title Checking checkpoint files (.pth)\n",
        "import os\n",
        "import time\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(INPUT_PATH, \"ckpt\")\n",
        "print(CHECKPOINT_DIR)\n",
        "# Create the checkpoint directory\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "# Wait loop to check if files exist\n",
        "required_files = [\"unet.pth\", \"content_encoder.pth\", \"style_encoder.pth\"]\n",
        "\n",
        "while True:\n",
        "    missing = [f for f in required_files if not os.path.exists(f\"{CHECKPOINT_DIR}/{f}\")]\n",
        "\n",
        "    if not missing:\n",
        "        print(\"\\n‚úÖ All weights found! You can proceed to the next step.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Waiting for files... Missing: {missing}\")\n",
        "        print(\"Upload them to the 'ckpt' folder now.\")\n",
        "        time.sleep(10) # Checks every 10 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Mx5uS5WQaqdn",
        "outputId": "2c444cdb-442d-4068-e687-90a63f39fe95"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-691170046.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnom_tu_tao_300_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{INPUT_PATH}/Ds_300_ChuNom_TuTao.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnom_tu_tao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnom_tu_tao_300_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUTPUT_PATH}/nom_tu_tao.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "nom_tu_tao_300_df = pd.read_csv(f\"{INPUT_PATH}/Ds_300_ChuNom_TuTao.csv\")\n",
        "nom_tu_tao = nom_tu_tao_300_df['word'].tolist()\n",
        "\n",
        "with open(f\"{OUTPUT_PATH}/nom_tu_tao.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(nom_tu_tao))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gma02BZvhx8I"
      },
      "outputs": [],
      "source": [
        "%cd {OUTPUT_PATH}\n",
        "!python FontDiffusion/batch_sample_evaluate.py \\\n",
        "    --characters f\"{INPUT_PATH}/nom_tu_tao.txt\" \\\n",
        "    --style_images f\"{INPUT_PATH}/FontDiffusion/DVSKTT_ref.jpg\" \\\n",
        "    --ckpt_dir \"ckpt/\" \\\n",
        "    --output_dir f\"{OUTPUT_PATH}/FontDiffusion/data_examples/train\" \\\n",
        "    --batch_size 8 \\\n",
        "    --num_inference_steps 20 \\\n",
        "    --fp16 \\\n",
        "    --enable_xformers \\\n",
        "    --enable_attention_slicing \\\n",
        "    --channels_last \\\n",
        "    --compile \\\n",
        "    --use_wandb \\\n",
        "    --wandb_project \"my-fontdiffuser\" \\\n",
        "    --wandb_run_name \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTz9WZ9ylBZx"
      },
      "outputs": [],
      "source": [
        "# @title Zipping the results folder\n",
        "!zip -r {OUTPUT_PATH}/output_data.zip {OUTPUT_PATH}/FontDiffusion/data_examples/train\n",
        "print(f\"Finish zipped the output data, ready for downloading\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIH9c0l-mRqB"
      },
      "outputs": [],
      "source": [
        "# @title Happy Christmas‚ú®\n",
        "# !rm -r -f FontDiffusion"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}