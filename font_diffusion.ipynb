{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":127.636509,"end_time":"2025-12-30T18:55:25.961447","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-12-30T18:53:18.324938","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a95a46ef","cell_type":"code","source":"# @title Environment Setup\nimport os\nimport sys\nif 'MPLBACKEND' in os.environ:\n    del os.environ['MPLBACKEND']\n    print(\"MPLBACKEND environment variable cleared.\")\n\n# 2. Clone the repository\n!rm -rf FontDiffusion\n!git clone https://github.com/dzungphieuluuky/FontDiffusion.git","metadata":{"id":"a95a46ef","outputId":"d76d28cd-6292-42bf-fffa-a8c7efb86ed0","papermill":{"duration":12.857369,"end_time":"2025-12-30T18:53:35.066181","exception":false,"start_time":"2025-12-30T18:53:22.208812","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:26:13.857415Z","iopub.execute_input":"2026-01-02T05:26:13.857786Z","iopub.status.idle":"2026-01-02T05:26:25.687415Z","shell.execute_reply.started":"2026-01-02T05:26:13.857754Z","shell.execute_reply":"2026-01-02T05:26:25.686681Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'FontDiffusion'...\nremote: Enumerating objects: 20623, done.\u001b[K\nremote: Counting objects: 100% (197/197), done.\u001b[K\nremote: Compressing objects: 100% (142/142), done.\u001b[K\nremote: Total 20623 (delta 126), reused 116 (delta 54), pack-reused 20426 (from 2)\u001b[K\nReceiving objects: 100% (20623/20623), 278.02 MiB | 38.29 MiB/s, done.\nResolving deltas: 100% (972/972), done.\nUpdating files: 100% (141/141), done.\n","output_type":"stream"}],"execution_count":13},{"id":"9cdd8666","cell_type":"code","source":"import os\nimport sys\nfrom IPython import get_ipython\nfrom typing import Optional\n\ndef configure_environment_paths():\n    try:\n        if \"google.colab\" in str(get_ipython()):\n            print(\"‚úÖ Environment: Google Colab\")\n            base_data_path = \"/content/\"\n            base_output_path = \"/content/\"\n            environment_name = \"colab\"\n        elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n            print(\"‚úÖ Environment: Kaggle\")\n            base_data_path = \"/kaggle/input/\"\n            base_output_path = \"/kaggle/working/\"\n            environment_name = \"kaggle\"\n        else:\n            print(\"‚ö†Ô∏è Environment: Local/Unknown\")\n            base_data_path = \"./data/\"\n            base_output_path = \"./output/\"\n            environment_name = \"local\"\n    except NameError:\n        print(\"‚ö†Ô∏è Non-interactive session. Using local paths.\")\n        base_data_path = \"./data/\"\n        base_output_path = \"./output/\"\n        environment_name = \"local\"\n    os.makedirs(base_output_path, exist_ok=True)\n    print(f\"üìÇ Data Path: {base_data_path}\")\n    print(f\"üì¶ Output Path: {base_output_path}\")\n    return base_data_path, base_output_path, environment_name\n\ndef load_secret(key_name: str) -> Optional[str]:\n    env = ENV_NAME\n    secret_value = None\n    print(f\"Attempting to load secret '{key_name}' from '{env}' environment...\")\n    try:\n        if env == \"colab\":\n            from google.colab import userdata\n            secret_value = userdata.get(key_name)\n        elif env == \"kaggle\":\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            secret_value = user_secrets.get_secret(key_name)\n        else:\n            secret_value = os.getenv(key_name)\n        if not secret_value:\n            print(f\"‚ö†Ô∏è Secret '{key_name}' not found in the {env} environment.\")\n            return None\n        print(f\"‚úÖ Successfully loaded secret '{key_name}'.\")\n        return secret_value\n    except Exception as e:\n        print(f\"‚ùå An error occurred while loading secret '{key_name}': {e}\")\n        return None\n\ndef print_system_info():\n    print(\"\\nüîß System Information\")\n    print(f\"Python version: {sys.version.split()[0]}\")\n    try:\n        import torch\n        print(f\"PyTorch version: {torch.__version__}\")\n        if torch.cuda.is_available():\n            print(f\"CUDA version: {torch.version.cuda}\")\n            print(f\"GPU count: {torch.cuda.device_count()}\")\n            for i in range(torch.cuda.device_count()):\n                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        else:\n            print(\"CUDA not available\")\n    except ImportError:\n        print(\"PyTorch not installed\")\n    finally:\n      !nvidia-smi\n\nINPUT_PATH, OUTPUT_PATH, ENV_NAME = configure_environment_paths()\nis_kaggle = (\"kaggle\" in ENV_NAME)\nis_colab = not is_kaggle\nprint_system_info()\n\nos.environ[\"WANDB_API_KEY\"] = wandb_key = load_secret(\"WANDB_API_KEY\")\nos.environ[\"HF_TOKEN\"] = HF_TOKEN = load_secret('HF_TOKEN')\n\n# Now, these libraries will log in automatically\nimport wandb\nimport huggingface_hub\n\nwandb.login() \nhuggingface_hub.login(token=os.environ[\"HF_TOKEN\"]) ","metadata":{"id":"9cdd8666","outputId":"8834f4e4-fc28-455c-a66c-d15b00de080a","papermill":{"duration":0.019157,"end_time":"2025-12-30T18:53:35.092303","exception":false,"start_time":"2025-12-30T18:53:35.073146","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:14:29.369466Z","iopub.execute_input":"2026-01-02T05:14:29.369679Z","iopub.status.idle":"2026-01-02T05:14:47.120609Z","shell.execute_reply.started":"2026-01-02T05:14:29.369657Z","shell.execute_reply":"2026-01-02T05:14:47.119964Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Environment: Kaggle\nüìÇ Data Path: /kaggle/input/\nüì¶ Output Path: /kaggle/working/\n\nüîß System Information\nPython version: 3.11.13\nPyTorch version: 2.6.0+cu124\nCUDA version: 12.4\nGPU count: 2\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\nFri Jan  2 05:14:35 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   47C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   49C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nAttempting to load secret 'WANDB_API_KEY' from 'kaggle' environment...\n‚úÖ Successfully loaded secret 'WANDB_API_KEY'.\nAttempting to load secret 'HF_TOKEN' from 'kaggle' environment...\n‚úÖ Successfully loaded secret 'HF_TOKEN'.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdungngocpham171\u001b[0m (\u001b[33mdungngocpham171-university-of-science\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\nNote: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n","output_type":"stream"}],"execution_count":2},{"id":"a73b4150","cell_type":"code","source":"!uv pip install --upgrade pip\n# 3. Install PyTorch 1.13\n%cd {OUTPUT_PATH}\n# Force reinstall torch 1.13 to match the model's training environment\n# !uv pip uninstall torch torchvision\n# !uv pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n!uv pip install torch==2.9 torchvision\n# 4. Install other dependencies\n\nprint(\"\\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\")\n# Install xformers compatible with Torch 1.13\n!uv pip install xformers==0.0.16 -q\n\n# Install original dependencies\n!uv pip install transformers==4.33.1 accelerate==0.23.0 diffusers==0.22.0\n!uv pip install gradio==4.8.0 pyyaml pygame opencv-python info-nce-pytorch kornia\n# -----------------------------------------------------------------\n!uv pip install lpips scikit-image pytorch-fid\n!sudo apt-get update && sudo apt-get install dos2unix\n!uv pip install gdown tqdm\n!uv pip install wandb\n!uv pip install --upgrade pyarrow datasets\nprint(\"\\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\")","metadata":{"id":"a73b4150","outputId":"97db2cec-8e2d-438b-e5f8-38df08b7f59e","papermill":{"duration":61.239828,"end_time":"2025-12-30T18:54:36.338205","exception":false,"start_time":"2025-12-30T18:53:35.098377","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:14:47.121331Z","iopub.execute_input":"2026-01-02T05:14:47.121672Z","iopub.status.idle":"2026-01-02T05:16:37.721603Z","shell.execute_reply.started":"2026-01-02T05:14:47.121651Z","shell.execute_reply":"2026-01-02T05:16:37.720740Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 265ms\u001b[0m\u001b[0m                                          \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 133ms\u001b[0m\u001b[0m                                              \n\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 252ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m                                 \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==24.1.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n/kaggle/working\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m40 packages\u001b[0m \u001b[2min 556ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m19 packages\u001b[0m \u001b[2min 38.65s\u001b[0m\u001b[0m                                           \n\u001b[2mUninstalled \u001b[1m17 packages\u001b[0m \u001b[2min 1.47s\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m19 packages\u001b[0m \u001b[2min 285ms\u001b[0m\u001b[0m                              \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.21.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.13.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.21.0+cu124 (from https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.2.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n\n‚¨áÔ∏è Installing Dependencies (Manually fixed)...\n  \u001b[31m√ó\u001b[0m Failed to build `xformers==0.0.16`\n\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n\u001b[31m      \u001b[0mstatus: 1)\n\n\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n\u001b[31m      \u001b[0mError in sitecustomize; set PYTHONVERBOSE for traceback:\n\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'wrapt'\n\u001b[31m      \u001b[0mTraceback (most recent call last):\n\u001b[31m      \u001b[0m  File \"<string>\", line 14, in <module>\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmphESZn7/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 331, in get_requires_for_build_wheel\n\u001b[31m      \u001b[0m    return self._get_build_requires(config_settings, requirements=[])\n\u001b[31m      \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmphESZn7/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 301, in _get_build_requires\n\u001b[31m      \u001b[0m    self.run_setup()\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmphESZn7/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 512, in run_setup\n\u001b[31m      \u001b[0m    super().run_setup(setup_script=setup_script)\n\u001b[31m      \u001b[0m  File\n\u001b[31m      \u001b[0m\"/root/.cache/uv/builds-v0/.tmphESZn7/lib/python3.11/site-packages/setuptools/build_meta.py\",\n\u001b[31m      \u001b[0mline 317, in run_setup\n\u001b[31m      \u001b[0m    exec(code, locals())\n\u001b[31m      \u001b[0m  File \"<string>\", line 23, in <module>\n\u001b[31m      \u001b[0mModuleNotFoundError: No module named 'torch'\n\n\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This error likely indicates that `\u001b[36mxformers@0.0.16\u001b[39m` depends\n\u001b[31m      \u001b[0mon `\u001b[36mtorch\u001b[39m`, but doesn't declare it as a build dependency. If\n\u001b[31m      \u001b[0m`\u001b[36mxformers\u001b[39m` is a first-party package, consider adding `\u001b[36mtorch\u001b[39m` to its\n\u001b[31m      \u001b[0m`\u001b[32mbuild-system.requires\u001b[39m`. Otherwise, `\u001b[32muv pip install torch\u001b[39m` into the\n\u001b[31m      \u001b[0menvironment and re-run with `\u001b[32m--no-build-isolation\u001b[39m`.\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 273ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 491ms\u001b[0m\u001b[0m                                             \n\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 930ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 41ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.9.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==0.23.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.34.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.22.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.13.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.33.1\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m102 packages\u001b[0m \u001b[2min 484ms\u001b[0m\u001b[0m                                       \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m8 packages\u001b[0m \u001b[2min 1.14s\u001b[0m\u001b[0m                                             \n\u001b[2mUninstalled \u001b[1m7 packages\u001b[0m \u001b[2min 623ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 78ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==5.38.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==4.8.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==1.11.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==0.7.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1minfo-nce-pytorch\u001b[0m\u001b[2m==0.1.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==2.1.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.12.0.88\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mopencv-python\u001b[0m\u001b[2m==4.11.0.86\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.13.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.12.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==11.0.3\u001b[0m\n\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `typer==0.16.0` does not have an extra named `all`\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m49 packages\u001b[0m \u001b[2min 192ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m                                              \n\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m                                 \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlpips\u001b[0m\u001b[2m==0.1.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpytorch-fid\u001b[0m\u001b[2m==0.3.0\u001b[0m\nHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \nGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,225 kB]\nGet:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,860 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,966 kB]\nGet:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,573 kB] \nHit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\nGet:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\nHit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\nGet:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\nGet:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\nGet:21 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:22 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\nGet:23 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\nGet:24 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\nFetched 38.5 MB in 3s (13.5 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  dos2unix\n0 upgraded, 1 newly installed, 0 to remove and 192 not upgraded.\nNeed to get 384 kB of archives.\nAfter this operation, 1,367 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dos2unix amd64 7.4.2-2 [384 kB]\nFetched 384 kB in 0s (1,634 kB/s)\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\ndebconf: falling back to frontend: Readline\nSelecting previously unselected package dos2unix.\n(Reading database ... 128639 files and directories currently installed.)\nPreparing to unpack .../dos2unix_7.4.2-2_amd64.deb ...\nUnpacking dos2unix (7.4.2-2) ...\nSetting up dos2unix (7.4.2-2) ...\nProcessing triggers for man-db (2.10.2-1) ...\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 111ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 117ms\u001b[0m\u001b[0m\n\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m39 packages\u001b[0m \u001b[2min 268ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m12 packages\u001b[0m \u001b[2min 436ms\u001b[0m\u001b[0m                                            \n\u001b[2mUninstalled \u001b[1m12 packages\u001b[0m \u001b[2min 557ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m12 packages\u001b[0m \u001b[2min 16.26s\u001b[0m\u001b[0m                             \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.10.5\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.4.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.4.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==19.0.1\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.20.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.21.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.3\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.2\u001b[0m\n\n‚úÖ Environment setup complete. You can now proceed to Block 2 (Inference).\n","output_type":"stream"}],"execution_count":3},{"id":"bd517dfe","cell_type":"code","source":"# KAGGLE CELL #1: Download checkpoint\nif is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\nimport os\nimport sys\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nos.chdir(OUTPUT_PATH)\n# Download from Hub\nif not os.path.exists(\"ckpt\") or not list(Path(\"ckpt\").glob(\"*.safetensors\")):\n    print(\"üì• Downloading checkpoint from Hugging Face Hub...\\n\")\n    from huggingface_hub import snapshot_download\n    snapshot_download(\n        repo_id=\"dzungpham/font-diffusion-weights\",\n        local_dir=\"ckpt\",\n        allow_patterns=\"*.safetensors\",\n        force_download=False\n    )\n    print(\"\\n‚úÖ Download complete!\")\nelse:\n    print(\"‚úÖ Checkpoint already downloaded\")\n# Verify\nprint(\"\\nüìÇ Files in ckpt/:\")\nfor file in os.listdir(\"ckpt\"):\n    if file.endswith(\".safetensors\"):\n        size = os.path.getsize(f\"ckpt/{file}\") / (1024**2)\n        print(f\"  ‚úì {file} ({size:.2f} MB)\")","metadata":{"id":"bd517dfe","outputId":"d83605e9-f5dc-4862-d1c9-b138a96ca47a","papermill":{"duration":12.524295,"end_time":"2025-12-30T18:54:48.878013","exception":false,"start_time":"2025-12-30T18:54:36.353718","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:16:37.722730Z","iopub.execute_input":"2026-01-02T05:16:37.722972Z","iopub.status.idle":"2026-01-02T05:16:41.622021Z","shell.execute_reply.started":"2026-01-02T05:16:37.722946Z","shell.execute_reply":"2026-01-02T05:16:41.621310Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 149ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 423ms\u001b[0m\u001b[0m                                             \n\u001b[2mUninstalled \u001b[1m4 packages\u001b[0m \u001b[2min 42ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m                                \u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.12.0\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.25.2\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.8\u001b[0m\nüì• Downloading checkpoint from Hugging Face Hub...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f9cba5a5e9466b9fcc5551aca1bedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet.safetensors:   0%|          | 0.00/315M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"702cdf51e6eb44aa9847f438986915b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"content_encoder.safetensors:   0%|          | 0.00/4.76M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9066d591e41747ed8216e6ac73c45955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"style_encoder.safetensors:   0%|          | 0.00/82.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993115ed0da14adda89d853eb77e47c9"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Download complete!\n\nüìÇ Files in ckpt/:\n  ‚úì unet.safetensors (300.34 MB)\n  ‚úì content_encoder.safetensors (4.54 MB)\n  ‚úì style_encoder.safetensors (78.58 MB)\n","output_type":"stream"}],"execution_count":4},{"id":"767e8ea2","cell_type":"code","source":"# @title Unzipping all archived files\nimport os\nimport glob\nfrom zipfile import ZipFile\n\nzip_file_paths = glob.glob(os.path.join(INPUT_PATH, '*.zip'))\n\nif not zip_file_paths:\n    print(f'No .zip files found in {INPUT_PATH}.')\nelse:\n    for zip_file_path in zip_file_paths:\n        if os.path.exists(zip_file_path):\n            print(f'Unzipping {zip_file_path}...')\n            !unzip -o {zip_file_path} -d ./\n            print(f'Unzipping of {zip_file_path} complete.')\n        else:\n            print(f'Error: The file {zip_file_path} was not found (post-glob check).')","metadata":{"id":"767e8ea2","outputId":"20185e27-e772-4823-e6bc-d9bd6d0b39a1","papermill":{"duration":0.023805,"end_time":"2025-12-30T18:54:48.917163","exception":false,"start_time":"2025-12-30T18:54:48.893358","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:16:41.622938Z","iopub.execute_input":"2026-01-02T05:16:41.623190Z","iopub.status.idle":"2026-01-02T05:16:41.630365Z","shell.execute_reply.started":"2026-01-02T05:16:41.623157Z","shell.execute_reply":"2026-01-02T05:16:41.629315Z"}},"outputs":[{"name":"stdout","text":"No .zip files found in /kaggle/input/.\n","output_type":"stream"}],"execution_count":5},{"id":"51941368","cell_type":"code","source":"import pandas as pd\nimport os\ndef convert_csv_to_chars_txt(input_csv_path: str, output_txt_path: str, column_name: str = 'word'):\n    \"\"\"\n    Reads a CSV file, extracts text from a specified column, and writes each character\n    to a new line in a plain text file.\n    Args:\n        input_csv_path (str): The full path to the input CSV file.\n        output_txt_path (str): The full path for the output text file.\n        column_name (str): The name of the column in the CSV file containing the text.\n    \"\"\"\n    if not os.path.exists(input_csv_path):\n        print(f\"Error: Input CSV file not found at '{input_csv_path}'. Please ensure the file is uploaded.\")\n        return\n    try:\n        df = pd.read_csv(input_csv_path)\n    except Exception as e:\n        print(f\"Error reading CSV file '{input_csv_path}': {e}\")\n        return\n    if column_name not in df.columns:\n        print(f\"Error: Column '{column_name}' not found in the CSV file '{input_csv_path}'.\")\n        return\n    all_characters = []\n    for item in df[column_name].astype(str).dropna().tolist():\n        for char in item:\n            all_characters.append(char)\n    os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(all_characters))\n    print(f\"Successfully converted '{input_csv_path}' to '{output_txt_path}', with one character per line.\")\nprint(\"\\n--- Demonstrating function with a dummy CSV file ---\")\ndummy_csv_path = os.path.join(OUTPUT_PATH, \"dummy_data.csv\")\ndummy_output_txt_path = os.path.join(OUTPUT_PATH, \"dummy_chars.txt\")\ndummy_data = {'word': ['hello', 'world', 'python']}\npd.DataFrame(dummy_data).to_csv(dummy_csv_path, index=False)\nprint(f\"Created a dummy CSV file at: {dummy_csv_path}\")\nconvert_csv_to_chars_txt(dummy_csv_path, dummy_output_txt_path)","metadata":{"id":"51941368","outputId":"2a2c352c-968a-4e4d-b4cc-88a02c7eb788","papermill":{"duration":1.62157,"end_time":"2025-12-30T18:54:50.594793","exception":false,"start_time":"2025-12-30T18:54:48.973223","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:16:41.631440Z","iopub.execute_input":"2026-01-02T05:16:41.631683Z","iopub.status.idle":"2026-01-02T05:16:43.039939Z","shell.execute_reply.started":"2026-01-02T05:16:41.631657Z","shell.execute_reply":"2026-01-02T05:16:43.039110Z"}},"outputs":[{"name":"stdout","text":"\n--- Demonstrating function with a dummy CSV file ---\nCreated a dummy CSV file at: /kaggle/working/dummy_data.csv\nSuccessfully converted '/kaggle/working/dummy_data.csv' to '/kaggle/working/dummy_chars.txt', with one character per line.\n","output_type":"stream"}],"execution_count":6},{"id":"4f4cf20b","cell_type":"code","source":"print(\"Model files:\")\n!ls -larth {OUTPUT_PATH}/ckpt","metadata":{"id":"4f4cf20b","outputId":"335f4192-47e7-451a-e14f-e0bd69fbdfc9","papermill":{"duration":0.140282,"end_time":"2025-12-30T18:54:50.749810","exception":false,"start_time":"2025-12-30T18:54:50.609528","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2026-01-02T05:16:43.042072Z","iopub.execute_input":"2026-01-02T05:16:43.042588Z","iopub.status.idle":"2026-01-02T05:16:43.182655Z","shell.execute_reply.started":"2026-01-02T05:16:43.042561Z","shell.execute_reply":"2026-01-02T05:16:43.181627Z"}},"outputs":[{"name":"stdout","text":"Model files:\ntotal 384M\ndrwxr-xr-x 3 root root 4.0K Jan  2 05:16 .cache\n-rw-r--r-- 1 root root 4.6M Jan  2 05:16 content_encoder.safetensors\n-rw-r--r-- 1 root root  79M Jan  2 05:16 style_encoder.safetensors\n-rw-r--r-- 1 root root 301M Jan  2 05:16 unet.safetensors\ndrwxr-xr-x 3 root root 4.0K Jan  2 05:16 .\ndrwxr-xr-x 5 root root 4.0K Jan  2 05:16 ..\n","output_type":"stream"}],"execution_count":7},{"id":"92cff682","cell_type":"code","source":"%cd {OUTPUT_PATH}\n# ==========================================\n# EXPORT / DOWNLOAD DATASET COMMANDS\n# ==========================================\nHF_USERNAME = \"dzungpham\"\n# Train Split\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train_original\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token HF_TOKEN\n\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/train\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token HF_TOKEN\n# Validation: Unseen Both\n!python FontDiffusion/export_hf_dataset_to_disk.py \\\n  --output_dir \"my_dataset/val\" \\\n  --repo_id {HF_USERNAME}/font-diffusion-generated-data \\\n  --split \"val\" \\\n  --token HF_TOKEN","metadata":{"id":"92cff682","papermill":{"duration":0.104394,"end_time":"2025-12-30T18:54:50.869230","exception":false,"start_time":"2025-12-30T18:54:50.764836","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T05:16:43.183962Z","iopub.execute_input":"2026-01-02T05:16:43.184651Z","iopub.status.idle":"2026-01-02T05:18:12.853795Z","shell.execute_reply.started":"2026-01-02T05:16:43.184622Z","shell.execute_reply":"2026-01-02T05:18:12.852983Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: train_original\nREADME.md: 3.05kB [00:00, 7.88MB/s]\ntrain_original-00000-of-00001.parquet: 100%|‚ñà‚ñà| 117M/117M [00:00<00:00, 208MB/s]\ntrain-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77.1M/77.1M [00:00<00:00, 195MB/s]\nval-00000-of-00001.parquet: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.65M/3.65M [00:00<00:00, 120MB/s]\nGenerating train_original split: 100%|‚ñà| 8985/8985 [00:00<00:00, 29701.92 exampl\nGenerating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 5760/5760 [00:00<00:00, 27396.65 examples/s]\nGenerating val split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 357/357 [00:00<00:00, 35506.18 examples/s]\n‚úì Loaded dataset with 8985 samples from Hub\n\nExporting images from dataset...\n\nüé® Exporting images...\nExporting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8985/8985 [00:21<00:00, 419.37it/s]\n‚úì Exported 599 content images\n‚úì Exported 8985 target images\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (8985 generations)\n\nüìä Metadata Statistics:\n  Total generations: 8985\n  Total characters: 599\n  Total styles: 15\n  Fonts: NomNaTong-Regular\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/train_original/ContentImage/\n  ‚úì my_dataset/train_original/TargetImage/\n  ‚úì my_dataset/train_original/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: train\n‚úì Loaded dataset with 5760 samples from Hub\n\nExporting images from dataset...\n\nüé® Exporting images...\nExporting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5760/5760 [00:13<00:00, 413.85it/s]\n‚úì Exported 480 content images\n‚úì Exported 5760 target images\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (5760 generations)\n\nüìä Metadata Statistics:\n  Total generations: 5760\n  Total characters: 480\n  Total styles: 12\n  Fonts: NomNaTong-Regular\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/train/ContentImage/\n  ‚úì my_dataset/train/TargetImage/\n  ‚úì my_dataset/train/results_checkpoint.json\n\n============================================================\nEXPORTING DATASET TO DISK\n============================================================\n\nüì• Loading dataset from Hub...\n   Repository: dzungpham/font-diffusion-generated-data\n   Split: val\n‚úì Loaded dataset with 357 samples from Hub\n\nExporting images from dataset...\n\nüé® Exporting images...\nExporting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 357/357 [00:00<00:00, 364.52it/s]\n‚úì Exported 119 content images\n‚úì Exported 357 target images\n\nüíæ Saving results_checkpoint.json...\n  ‚úì Saved results_checkpoint.json (357 generations)\n\nüìä Metadata Statistics:\n  Total generations: 357\n  Total characters: 119\n  Total styles: 3\n  Fonts: NomNaTong-Regular\n\n============================================================\n‚úÖ EXPORT COMPLETE!\n============================================================\n\nFiles created:\n  ‚úì my_dataset/val/ContentImage/\n  ‚úì my_dataset/val/TargetImage/\n  ‚úì my_dataset/val/results_checkpoint.json\n","output_type":"stream"}],"execution_count":8},{"id":"6db9c1d6-dd60-479c-92c4-2f653e4d48fd","cell_type":"code","source":"print(\"Fonts currently in fonts/ folder\")\n!ls -lt FontDiffusion/fonts\nprint(\"Styles in style_images/ folder\")\n!ls -l FontDiffusion/styles_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T05:18:12.854971Z","iopub.execute_input":"2026-01-02T05:18:12.855308Z","iopub.status.idle":"2026-01-02T05:18:13.117852Z","shell.execute_reply.started":"2026-01-02T05:18:12.855282Z","shell.execute_reply":"2026-01-02T05:18:13.117096Z"}},"outputs":[{"name":"stdout","text":"Fonts currently in fonts/ folder\ntotal 332584\n-rw-r--r-- 1 root root 26929728 Jan  2 05:14  NomNaTongLight2.ttf\n-rw-r--r-- 1 root root 14574480 Jan  2 05:14  NomNaTongLight.ttf\n-rw-r--r-- 1 root root 31729820 Jan  2 05:14  NomNaTong-Regular2.otf\n-rw-r--r-- 1 root root 14574552 Jan  2 05:14  NomNaTong-Regular.ttf\n-rw-r--r-- 1 root root  9424552 Jan  2 05:14  NomNaTong-Regular.otf\n-rw-r--r-- 1 root root 12967288 Jan  2 05:14  HanaMinC.otf\n-rw-r--r-- 1 root root 30739236 Jan  2 05:14  HanaMinB.ttf\n-rw-r--r-- 1 root root 32201032 Jan  2 05:14  HanaMinB.otf\n-rw-r--r-- 1 root root 22761228 Jan  2 05:14  HanaMinA.ttf\n-rw-r--r-- 1 root root 31621108 Jan  2 05:14  HanaMinA.otf\n-rw-r--r-- 1 root root 18202176 Jan  2 05:14 'Han-nom Minh 1.42.otf'\n-rw-r--r-- 1 root root 19505228 Jan  2 05:14  Han-Nom-Khai-Regular-300623.ttf\n-rw-r--r-- 1 root root 20368044 Jan  2 05:14 'Han-Nom Kai 1.00.otf'\n-rw-r--r-- 1 root root 33815824 Jan  2 05:14 'HAN NOM B.ttf'\n-rw-r--r-- 1 root root 21320444 Jan  2 05:14 'HAN NOM A.ttf'\nStyles in style_images/ folder\ntotal 520\n-rw-r--r-- 1 root root  55480 Jan  2 05:14 1.png\n-rw-r--r-- 1 root root  73193 Jan  2 05:14 2.png\n-rw-r--r-- 1 root root  62305 Jan  2 05:14 3.png\n-rw-r--r-- 1 root root  47202 Jan  2 05:14 4.png\n-rw-r--r-- 1 root root  40943 Jan  2 05:14 5.png\n-rw-r--r-- 1 root root  11400 Jan  2 05:14 6.png\n-rw-r--r-- 1 root root  26508 Jan  2 05:14 hanh.png\n-rw-r--r-- 1 root root   1569 Jan  2 05:14 hanhthu1.jpg\n-rw-r--r-- 1 root root   1036 Jan  2 05:14 hanhthu2.jpg\n-rw-r--r-- 1 root root 100710 Jan  2 05:14 khai.png\n-rw-r--r-- 1 root root  36429 Jan  2 05:14 le.png\n-rw-r--r-- 1 root root   1086 Jan  2 05:14 lethu1.jpg\n-rw-r--r-- 1 root root   1182 Jan  2 05:14 lethu2.jpg\n-rw-r--r-- 1 root root  17600 Jan  2 05:14 thao.png\n-rw-r--r-- 1 root root  27078 Jan  2 05:14 trien.png\n","output_type":"stream"}],"execution_count":9},{"id":"29deed1d","cell_type":"code","source":"if is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\" \"protobuf<5.0.0\" \"numpy<2.0.0\"\n%cd {OUTPUT_PATH}\n!accelerate launch --num_processes 1 \\\n    FontDiffusion/sample_batch.py \\\n    --characters \"FontDiffusion/NomTuTao/Ds_10k_ChuNom_TuTao.txt\" \\\n    --style_images \"FontDiffusion/styles_images\" \\\n    --ckpt_dir \"ckpt/\" \\\n    --ttf_path \"FontDiffusion/fonts/NomNaTong-Regular.otf\" \\\n    --output_dir \"my_dataset/train_original\" \\\n    --num_inference_steps 20 \\\n    --guidance_scale 7.5 \\\n    --start_line 301 \\\n    --end_line 600 \\\n    --batch_size 35 \\\n    --save_interval 1 \\\n    --channels_last \\\n    --seed 42 \\\n    --compile \\\n    --enable_xformers","metadata":{"id":"29deed1d","outputId":"749b50d0-75e3-4d36-e509-919188feb64c","papermill":{"duration":10.53661,"end_time":"2025-12-30T18:55:01.421093","exception":false,"start_time":"2025-12-30T18:54:50.884483","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"997103e5-221c-40a1-a0d1-83a93e1030f7","cell_type":"code","source":"!find my_dataset/train_original/ContentImage -type f | wc -l\n!find my_dataset/train_original/TargetImage -type f | wc -l","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"619d3b79-f33c-41c6-be25-e80b2c1165a5","cell_type":"code","source":"# !ls -lt my_dataset/train_original/ContentImage/*\n# !ls -l my_dataset/train_original/TargetImage/*hanhthu*\n# !ls -lt my_dataset/train_original/TargetImage/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"97c4a6a0-dba9-46be-b6df-d79cce0df689","cell_type":"code","source":"import re\nfrom pathlib import Path\n\n# Your valid pattern\nexpected_pattern = r\"U\\+[0-9A-F]{4,5}.*_[0-9a-f]{8}\\.png\"\n\n# Define the root directory ('.' for current directory)\nroot_dir = Path('./my_dataset/train_original/TargetImage')\n\n# .rglob('*') finds every file recursively\nfor path in root_dir.rglob('*'):\n    # Process only files (ignore directories)\n    if path.is_file():\n        # Check if the FILENAME (path.name) matches the regex\n        if not re.match(expected_pattern, path.name):\n            try:\n                print(f\"Deleting invalid file: {path}\")\n                # path.unlink() # This deletes the file\n            except Exception as e:\n                print(f\"Error deleting {path}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f9250a14","cell_type":"code","source":"!python FontDiffusion/create_validation_split.py \\\n  --data_root my_dataset \\\n  --val_ratio 0.2 \\\n  --seed 42","metadata":{"id":"f9250a14","outputId":"0f834d09-da00-4aa4-f486-6e70981b4137","papermill":{"duration":0.236541,"end_time":"2025-12-30T18:55:01.673705","exception":false,"start_time":"2025-12-30T18:55:01.437164","status":"completed"},"tags":[],"trusted":true,"colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"id":"79508d80-fac1-4318-9174-a32613a557e3","cell_type":"code","source":"!uv pip install --upgrade pyarrow datasets","metadata":{"trusted":true,"id":"79508d80-fac1-4318-9174-a32613a557e3"},"outputs":[],"execution_count":null},{"id":"48f97e84-cd8c-49a9-86bd-fce456be56a4","cell_type":"code","source":"# remove_unparseable_files.py\n\nwith open(\"my_dataset/unparseable_files.txt\", \"r\", encoding=\"utf-8\") as f:\n    paths = [line.strip() for line in f if line.strip()]\n\nimport os\n\nfor path in paths:\n    try:\n        if os.path.exists(path):\n            os.remove(path)\n            print(f\"Deleted: {path}\")\n        else:\n            print(f\"Not found: {path}\")\n    except Exception as e:\n        print(f\"Error deleting {path}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"vRL8QovYCvLY","cell_type":"code","source":"HF_USERNAME = \"dzungpham\"\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train_original\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"train_original\" \\\n  --token {HF_TOKEN}\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/train\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"train\" \\\n  --token {HF_TOKEN}\n\n# Train Split\n!python FontDiffusion/create_hf_dataset.py \\\n  --data_dir \"my_dataset/val\" \\\n  --repo_id dzungpham/font-diffusion-generated-data \\\n  --split \"val\" \\\n  --token {HF_TOKEN}\n","metadata":{"id":"vRL8QovYCvLY","outputId":"08301c52-4ae1-4268-c516-2ff8bd834783","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"outputs":[],"execution_count":null},{"id":"a87caab2","cell_type":"code","source":"import torch, gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"a87caab2","papermill":{"duration":1.992585,"end_time":"2025-12-30T18:55:24.769269","exception":false,"start_time":"2025-12-30T18:55:22.776684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"267634e8","cell_type":"code","source":"# TRAINING PHASE 1\nif is_colab:\n  !uv pip install --upgrade \"huggingface-hub>=0.34.0,<1.0\"\nelse:\n  !uv pip install --upgrade \"huggingface-hub==0.25.2\"\nimport wandb\n\nMAX_TRAIN_STEPS = 500\n!accelerate launch FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_1\" \\\n    --data_root=\"my_dataset\" \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_800\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    \\\n    --train_batch_size=8 \\\n    --gradient_accumulation_steps=2 \\\n    --perceptual_coefficient=0.05 \\\n    --offset_coefficient=0.7 \\\n    --max_train_steps={MAX_TRAIN_STEPS} \\\n    --ckpt_interval={MAX_TRAIN_STEPS // 2} \\\n    --log_interval=50 \\\n    \\\n    --learning_rate=1e-4 \\\n    --lr_scheduler=\"linear\" \\\n    --lr_warmup_steps=10000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n\nimport logging\nlogging.info(\"Finetuned model checkpoints:\")\n!ls -lr outputs/FontDiffuser","metadata":{"id":"267634e8","papermill":{"duration":0.021927,"end_time":"2025-12-30T18:55:24.807644","exception":false,"start_time":"2025-12-30T18:55:24.785717","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T06:09:05.690160Z","iopub.execute_input":"2026-01-02T06:09:05.690946Z","iopub.status.idle":"2026-01-02T06:09:28.055350Z","shell.execute_reply.started":"2026-01-02T06:09:05.690915Z","shell.execute_reply":"2026-01-02T06:09:28.054558Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 90ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2mAudited \u001b[1m12 packages\u001b[0m \u001b[2min 0.15ms\u001b[0m\u001b[0m\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\nThe following values were not passed to `accelerate launch` and had defaults used instead:\n\t`--num_processes` was set to a value of `2`\n\t\tMore than one GPU was found, enabling multi-GPU training.\n\t\tIf this was unintended please pass in `--num_processes=1`.\n\t`--num_machines` was set to a value of `1`\n\t`--mixed_precision` was set to a value of `'no'`\n\t`--dynamo_backend` was set to a value of `'no'`\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n2026-01-02 06:09:16.486648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767334156.512909    1710 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767334156.521425    1710 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-01-02 06:09:16.521764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767334156.545268    1709 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767334156.553060    1709 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\npygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\nHello from the pygame community. https://www.pygame.org/contribute.html\npygame 2.6.1 (SDL 2.28.4, Python 3.11.13)\nHello from the pygame community. https://www.pygame.org/contribute.html\nusage: my_train.py [-h] [--seed SEED] [--experience_name EXPERIENCE_NAME]\n                   [--data_root DATA_ROOT] [--output_dir OUTPUT_DIR]\n                   [--report_to REPORT_TO] [--logging_dir LOGGING_DIR]\n                   [--resolution RESOLUTION] [--unet_channels UNET_CHANNELS]\n                   [--style_image_size STYLE_IMAGE_SIZE]\n                   [--content_image_size CONTENT_IMAGE_SIZE]\n                   [--content_encoder_downsample_size CONTENT_ENCODER_DOWNSAMPLE_SIZE]\n                   [--channel_attn CHANNEL_ATTN]\n                   [--content_start_channel CONTENT_START_CHANNEL]\n                   [--style_start_channel STYLE_START_CHANNEL] [--phase_2]\n                   [--phase_1_ckpt_dir PHASE_1_CKPT_DIR]\n                   [--temperature TEMPERATURE] [--mode MODE]\n                   [--scr_image_size SCR_IMAGE_SIZE]\n                   [--scr_ckpt_path SCR_CKPT_PATH] [--num_neg NUM_NEG]\n                   [--nce_layers NCE_LAYERS] [--sc_coefficient SC_COEFFICIENT]\n                   [--train_batch_size TRAIN_BATCH_SIZE]\n                   [--perceptual_coefficient PERCEPTUAL_COEFFICIENT]\n                   [--offset_coefficient OFFSET_COEFFICIENT]\n                   [--max_train_steps MAX_TRAIN_STEPS]\n                   [--ckpt_interval CKPT_INTERVAL]\n                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                   [--log_interval LOG_INTERVAL]\n                   [--learning_rate LEARNING_RATE] [--scale_lr]\n                   [--lr_scheduler LR_SCHEDULER]\n                   [--lr_warmup_steps LR_WARMUP_STEPS] [--drop_prob DROP_PROB]\n                   [--beta_scheduler BETA_SCHEDULER] [--adam_beta1 ADAM_BETA1]\n                   [--adam_beta2 ADAM_BETA2]\n                   [--adam_weight_decay ADAM_WEIGHT_DECAY]\n                   [--adam_epsilon ADAM_EPSILON]\n                   [--max_grad_norm MAX_GRAD_NORM]\n                   [--mixed_precision {no,fp16,bf16}]\n                   [--algorithm_type ALGORITHM_TYPE]\n                   [--guidance_type GUIDANCE_TYPE]\n                   [--guidance_scale GUIDANCE_SCALE]\n                   [--num_inference_steps NUM_INFERENCE_STEPS]\n                   [--model_type MODEL_TYPE] [--order ORDER]\n                   [--skip_type SKIP_TYPE] [--method METHOD]\n                   [--correcting_x0_fn CORRECTING_X0_FN] [--t_start T_START]\n                   [--t_end T_END] [--local_rank LOCAL_RANK]\n                   [--val_interval VAL_INTERVAL]\nmy_train.py: error: argument --ckpt_interval: invalid int value: '250.0'\nusage: my_train.py [-h] [--seed SEED] [--experience_name EXPERIENCE_NAME]\n                   [--data_root DATA_ROOT] [--output_dir OUTPUT_DIR]\n                   [--report_to REPORT_TO] [--logging_dir LOGGING_DIR]\n                   [--resolution RESOLUTION] [--unet_channels UNET_CHANNELS]\n                   [--style_image_size STYLE_IMAGE_SIZE]\n                   [--content_image_size CONTENT_IMAGE_SIZE]\n                   [--content_encoder_downsample_size CONTENT_ENCODER_DOWNSAMPLE_SIZE]\n                   [--channel_attn CHANNEL_ATTN]\n                   [--content_start_channel CONTENT_START_CHANNEL]\n                   [--style_start_channel STYLE_START_CHANNEL] [--phase_2]\n                   [--phase_1_ckpt_dir PHASE_1_CKPT_DIR]\n                   [--temperature TEMPERATURE] [--mode MODE]\n                   [--scr_image_size SCR_IMAGE_SIZE]\n                   [--scr_ckpt_path SCR_CKPT_PATH] [--num_neg NUM_NEG]\n                   [--nce_layers NCE_LAYERS] [--sc_coefficient SC_COEFFICIENT]\n                   [--train_batch_size TRAIN_BATCH_SIZE]\n                   [--perceptual_coefficient PERCEPTUAL_COEFFICIENT]\n                   [--offset_coefficient OFFSET_COEFFICIENT]\n                   [--max_train_steps MAX_TRAIN_STEPS]\n                   [--ckpt_interval CKPT_INTERVAL]\n                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n                   [--log_interval LOG_INTERVAL]\n                   [--learning_rate LEARNING_RATE] [--scale_lr]\n                   [--lr_scheduler LR_SCHEDULER]\n                   [--lr_warmup_steps LR_WARMUP_STEPS] [--drop_prob DROP_PROB]\n                   [--beta_scheduler BETA_SCHEDULER] [--adam_beta1 ADAM_BETA1]\n                   [--adam_beta2 ADAM_BETA2]\n                   [--adam_weight_decay ADAM_WEIGHT_DECAY]\n                   [--adam_epsilon ADAM_EPSILON]\n                   [--max_grad_norm MAX_GRAD_NORM]\n                   [--mixed_precision {no,fp16,bf16}]\n                   [--algorithm_type ALGORITHM_TYPE]\n                   [--guidance_type GUIDANCE_TYPE]\n                   [--guidance_scale GUIDANCE_SCALE]\n                   [--num_inference_steps NUM_INFERENCE_STEPS]\n                   [--model_type MODEL_TYPE] [--order ORDER]\n                   [--skip_type SKIP_TYPE] [--method METHOD]\n                   [--correcting_x0_fn CORRECTING_X0_FN] [--t_start T_START]\n                   [--t_end T_END] [--local_rank LOCAL_RANK]\n                   [--val_interval VAL_INTERVAL]\nmy_train.py: error: argument --ckpt_interval: invalid int value: '250.0'\nE0102 06:09:26.735000 1701 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 2) local_rank: 0 (pid: 1709) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/local/bin/accelerate\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n    args.func(args)\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 977, in launch_command\n    multi_gpu_launcher(args)\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 646, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 927, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\nFontDiffusion/my_train.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2026-01-02_06:09:26\n  host      : a18cf9ad7484\n  rank      : 1 (local_rank: 1)\n  exitcode  : 2 (pid: 1710)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2026-01-02_06:09:26\n  host      : a18cf9ad7484\n  rank      : 0 (local_rank: 0)\n  exitcode  : 2 (pid: 1709)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\ntotal 28\ndrwxr-xr-x 2 root root 4096 Jan  2 06:04 global_step_800\ndrwxr-xr-x 2 root root 4096 Jan  2 05:51 global_step_400\ndrwxr-xr-x 2 root root 4096 Jan  2 05:34 global_step_200\ndrwxr-xr-x 2 root root 4096 Jan  2 05:31 global_step_100\n-rw-r--r-- 1 root root 1266 Jan  2 05:37 FontDiffuser_training_phase_1_config.yaml\n-rw-r--r-- 1 root root 6850 Jan  2 06:04 fontdiffuser_training.log\n","output_type":"stream"}],"execution_count":21},{"id":"2417efae-db7f-4e27-a203-cb1508bc87e6","cell_type":"code","source":"!ls -lt ckpt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T06:10:44.397909Z","iopub.execute_input":"2026-01-02T06:10:44.398722Z","iopub.status.idle":"2026-01-02T06:10:44.530253Z","shell.execute_reply.started":"2026-01-02T06:10:44.398688Z","shell.execute_reply":"2026-01-02T06:10:44.529450Z"}},"outputs":[{"name":"stdout","text":"content_encoder.safetensors  style_encoder.safetensors\tunet.safetensors\n","output_type":"stream"}],"execution_count":22},{"id":"97f8136e","cell_type":"code","source":"# TRAINING PHASE 2\n!wandb login\n!python FontDiffusion/my_train.py \\\n    --seed=123 \\\n    --experience_name=\"FontDiffuser_training_phase_2\" \\\n    --data_root=\"my_dataset\" \\\n    --output_dir=\"outputs/FontDiffuser\" \\\n    --report_to=\"wandb\" \\\n    --phase_2 \\\n    --phase_1_ckpt_dir=\"outputs/FontDiffuser/global_step_800\" \\\n    --scr_ckpt_path=\"ckpt/scr_210000.pth\" \\\n    --sc_coefficient=0.05 \\\n    --num_neg=13 \\\n    --resolution=96 \\\n    --style_image_size=96 \\\n    --content_image_size=96 \\\n    --content_encoder_downsample_size=3 \\\n    --channel_attn=True \\\n    --content_start_channel=64 \\\n    --style_start_channel=64 \\\n    --train_batch_size=8 \\\n    --perceptual_coefficient=0.03 \\\n    --offset_coefficient=0.4 \\\n    --max_train_steps=100 \\\n    --ckpt_interval=50 \\\n    --gradient_accumulation_steps=2 \\\n    --log_interval=50 \\\n    --learning_rate=1e-5 \\\n    --lr_scheduler=\"constant\" \\\n    --lr_warmup_steps=1000 \\\n    --drop_prob=0.1 \\\n    --mixed_precision=\"no\"\n","metadata":{"id":"97f8136e","papermill":{"duration":0.022471,"end_time":"2025-12-30T18:55:24.845778","exception":false,"start_time":"2025-12-30T18:55:24.823307","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"1f018250-05c6-490f-9e38-a1d28571190b","cell_type":"code","source":"!ls -l outputs/FontDiffuser/global_step_800","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T06:13:13.023168Z","iopub.execute_input":"2026-01-02T06:13:13.023609Z","iopub.status.idle":"2026-01-02T06:13:13.156558Z","shell.execute_reply.started":"2026-01-02T06:13:13.023574Z","shell.execute_reply":"2026-01-02T06:13:13.155809Z"}},"outputs":[{"name":"stdout","text":"total 785332\n-rw-r--r-- 1 root root   4756580 Jan  2 06:04 content_encoder.safetensors\n-rw-r--r-- 1 root root  82394556 Jan  2 06:04 style_encoder.safetensors\n-rw-r--r-- 1 root root 402084924 Jan  2 06:04 total_model.safetensors\n-rw-r--r-- 1 root root 314927748 Jan  2 06:04 unet.safetensors\n","output_type":"stream"}],"execution_count":25},{"id":"88c45e2f","cell_type":"code","source":"!python FontDiffusion/pth2safetensors.py \\\n    --weights_dir \"outputs/FontDiffuser/global_step_800\" \\\n    --repo_id \"dzungpham/font-diffusion-weights\" \\\n    --token \"{HF_TOKEN}\"","metadata":{"id":"88c45e2f","papermill":{"duration":0.217876,"end_time":"2025-12-30T18:55:25.079820","exception":false,"start_time":"2025-12-30T18:55:24.861944","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T06:13:20.713360Z","iopub.execute_input":"2026-01-02T06:13:20.714017Z","iopub.status.idle":"2026-01-02T06:13:36.533755Z","shell.execute_reply.started":"2026-01-02T06:13:20.713988Z","shell.execute_reply":"2026-01-02T06:13:36.532932Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nPYTORCH TO SAFETENSORS CONVERTER & HF UPLOADER\n======================================================================\n\n======================================================================\nVALIDATING INPUTS\n======================================================================\n‚úì Weights directory: outputs/FontDiffuser/global_step_800\n  Contents: 4 files\n‚úì Repository ID: dzungpham/font-diffusion-weights\n‚úì HF token: ********************\n\n‚úì Files to process: 5\n  ‚ö† content_encoder.pth (not found)\n  ‚ö† style_encoder.pth (not found)\n  ‚ö† unet.pth (not found)\n  ‚ö† total_model.pth (not found)\n  ‚ö† scr.pth (not found)\n\n======================================================================\nCONVERTING .pth TO SAFETENSORS\n======================================================================\n\n‚ö† content_encoder.pth: Not found, skipping\n\n‚ö† style_encoder.pth: Not found, skipping\n\n‚ö† unet.pth: Not found, skipping\n\n‚ö† total_model.pth: Not found, skipping\n\n‚ö† scr.pth: Not found, skipping\n\n----------------------------------------------------------------------\nConversion complete: 0 succeeded, 0 failed\n\n======================================================================\nUPLOADING TO HUGGING FACE HUB\n======================================================================\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n\nüì§ Creating/verifying repository...\n  Repo ID: dzungpham/font-diffusion-weights\n  Private: True\n‚úì Repository ready\n\nüì§ Uploading folder: outputs/FontDiffuser/global_step_800\n\nstyle_encoder.safetensors:   0%|                    | 0.00/82.4M [00:00<?, ?B/s]\u001b[A\n\ntotal_model.safetensors:   0%|                       | 0.00/402M [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nunet.safetensors:   0%|                              | 0.00/315M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\n\n\nUpload 4 LFS files:   0%|                                 | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nstyle_encoder.safetensors:   0%|           | 16.4k/82.4M [00:00<14:00, 98.1kB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:   0%|                      | 16.4k/315M [00:00<52:22, 100kB/s]\u001b[A\u001b[A\u001b[A\ncontent_encoder.safetensors:   0%|         | 16.4k/4.76M [00:00<00:53, 88.3kB/s]\u001b[A\n\n\nstyle_encoder.safetensors:   3%|‚ñé          | 2.65M/82.4M [00:00<00:08, 9.95MB/s]\u001b[A\u001b[A\u001b[A\ncontent_encoder.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñã    | 2.46M/4.76M [00:00<00:00, 9.31MB/s]\u001b[A\n\ntotal_model.safetensors:   1%|‚ñè             | 4.46M/402M [00:00<00:24, 16.1MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:   3%|‚ñã                    | 10.3M/315M [00:00<00:08, 34.7MB/s]\u001b[A\u001b[A\u001b[A\n\nstyle_encoder.safetensors:  12%|‚ñà‚ñé         | 9.96M/82.4M [00:00<00:02, 29.8MB/s]\u001b[A\u001b[A\n\nstyle_encoder.safetensors:  16%|‚ñà‚ñä         | 13.3M/82.4M [00:00<00:02, 29.6MB/s]\u001b[A\u001b[A\n\n\ncontent_encoder.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.76M/4.76M [00:00<00:00, 6.00MB/s]\u001b[A\u001b[A\u001b[A\nstyle_encoder.safetensors:  20%|‚ñà‚ñà‚ñè        | 16.5M/82.4M [00:00<00:03, 18.2MB/s]\n\ntotal_model.safetensors:   4%|‚ñå             | 16.9M/402M [00:00<00:22, 17.4MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:   6%|‚ñà‚ñè                   | 17.5M/315M [00:00<00:16, 17.9MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:   7%|‚ñâ             | 27.5M/402M [00:01<00:12, 28.9MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:   8%|‚ñà‚ñã                   | 25.4M/315M [00:01<00:11, 25.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nstyle_encoder.safetensors:  39%|‚ñà‚ñà‚ñà‚ñà‚ñé      | 32.0M/82.4M [00:01<00:01, 31.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nunet.safetensors:   9%|‚ñà‚ñâ                   | 28.5M/315M [00:01<00:12, 22.8MB/s]\u001b[A\u001b[A\u001b[A\n\nstyle_encoder.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 48.0M/82.4M [00:01<00:00, 39.5MB/s]\u001b[A\u001b[A\n\ntotal_model.safetensors:  10%|‚ñà‚ñç            | 41.8M/402M [00:01<00:10, 33.2MB/s]\u001b[A\u001b[A\n\n\nstyle_encoder.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 58.0M/82.4M [00:01<00:00, 45.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nstyle_encoder.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 62.7M/82.4M [00:01<00:00, 40.8MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  12%|‚ñà‚ñã            | 46.8M/402M [00:01<00:13, 26.9MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  14%|‚ñà‚ñà‚ñâ                  | 43.8M/315M [00:01<00:10, 26.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nstyle_encoder.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 66.9M/82.4M [00:02<00:00, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  13%|‚ñà‚ñä            | 50.7M/402M [00:02<00:16, 21.2MB/s]\u001b[A\u001b[A\n\n\nstyle_encoder.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 74.4M/82.4M [00:02<00:00, 29.7MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  16%|‚ñà‚ñà‚ñè           | 64.0M/402M [00:02<00:11, 30.2MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  20%|‚ñà‚ñà‚ñà‚ñà‚ñé                | 64.0M/315M [00:02<00:08, 30.0MB/s]\u001b[A\u001b[A\u001b[A\n\nstyle_encoder.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82.4M/82.4M [00:02<00:00, 27.9MB/s]\u001b[A\u001b[A\n\n\n\nunet.safetensors:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 80.0M/315M [00:02<00:06, 35.6MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  24%|‚ñà‚ñà‚ñà‚ñé          | 96.0M/402M [00:03<00:07, 42.6MB/s]\u001b[A\u001b[A\n\n\n\nUpload 4 LFS files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 2/4 [00:03<00:03,  1.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  28%|‚ñà‚ñà‚ñà‚ñà‚ñè          | 112M/402M [00:03<00:06, 45.4MB/s]\u001b[A\u001b[A\n\ntotal_model.safetensors:  29%|‚ñà‚ñà‚ñà‚ñà‚ñç          | 118M/402M [00:03<00:06, 46.9MB/s]\u001b[A\u001b[A\n\ntotal_model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñà‚ñå          | 124M/402M [00:03<00:06, 41.0MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç              | 96.0M/315M [00:03<00:07, 28.6MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  32%|‚ñà‚ñà‚ñà‚ñà‚ñä          | 128M/402M [00:04<00:09, 29.9MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 112M/315M [00:04<00:06, 32.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nunet.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 128M/315M [00:04<00:04, 39.0MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 144M/402M [00:04<00:06, 37.3MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 144M/315M [00:04<00:03, 43.5MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ         | 160M/402M [00:04<00:05, 42.6MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 160M/315M [00:04<00:03, 47.3MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 176M/402M [00:04<00:04, 46.3MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé         | 176M/315M [00:05<00:02, 50.3MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè       | 192M/402M [00:05<00:04, 44.9MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 192M/315M [00:05<00:02, 52.5MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä       | 208M/402M [00:05<00:04, 47.1MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 208M/315M [00:05<00:02, 51.4MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 224M/402M [00:05<00:03, 49.0MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 224M/315M [00:06<00:01, 54.2MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 240M/402M [00:06<00:03, 50.0MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 240M/315M [00:06<00:01, 54.3MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå     | 256M/402M [00:06<00:02, 51.6MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 256M/315M [00:06<00:01, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 272M/402M [00:06<00:02, 52.0MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 272M/315M [00:06<00:00, 55.9MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 288M/402M [00:07<00:02, 51.4MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 288M/315M [00:07<00:00, 56.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nunet.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 298M/315M [00:07<00:00, 55.1MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 304M/402M [00:07<00:01, 51.8MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 304M/315M [00:07<00:00, 47.4MB/s]\u001b[A\u001b[A\u001b[A\n\ntotal_model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 320M/402M [00:07<00:01, 51.0MB/s]\u001b[A\u001b[A\n\n\nunet.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 308M/315M [00:07<00:00, 36.7MB/s]\u001b[A\u001b[A\u001b[A\n\nunet.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 315M/315M [00:08<00:00, 39.0MB/s]\u001b[A\u001b[A\n\n\ntotal_model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 352M/402M [00:08<00:00, 52.7MB/s]\u001b[A\u001b[A\n\ntotal_model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 368M/402M [00:08<00:00, 51.5MB/s]\u001b[A\u001b[A\n\ntotal_model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 384M/402M [00:08<00:00, 50.3MB/s]\u001b[A\u001b[A\n\ntotal_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 402M/402M [00:09<00:00, 42.6MB/s]\u001b[A\u001b[A\n\n\n\n\nUpload 4 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n\n‚úì Upload successful!\n  Repository URL: https://huggingface.co/models/dzungpham/font-diffusion-weights\n\n======================================================================\n‚úì ALL DONE!\n======================================================================\n\nüì¶ Your weights are now available at:\n   https://huggingface.co/models/dzungpham/font-diffusion-weights\n\nüìñ Load them with:\n   from safetensors.torch import load_file\n   state = load_file('model.safetensors')\n","output_type":"stream"}],"execution_count":26},{"id":"5868b20b","cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\nfrom typing import List\ndef find_result_folders(base_path: Path, pattern_name: str) -> List[Path]:\n    return [p for p in base_path.glob(pattern_name) if p.is_dir()]\n\ndef zip_folder(folder_path: Path, output_base_path: Path) -> bool:\n    folder_name = folder_path.name\n    zip_path = output_base_path / f\"{folder_name}.zip\"\n    try:\n        print(f\"   -> Zipping folder: {folder_name}...\")\n        with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zipf:\n            for file_path in folder_path.rglob(\"*\"):\n                if file_path.is_file():\n                    arcname = file_path.relative_to(folder_path.parent)\n                    zipf.write(file_path, arcname)\n        print(f\"   ‚úÖ Created ZIP: {zip_path.name}\")\n        return True\n    except Exception as exc:\n        print(f\"   ‚ùå Failed to zip {folder_name}: {exc}\")\n        return False\n\ndef zip_stats_results_folders(output_base_path: str, pattern_name: str) -> None:\n    base = Path(output_base_path)\n    base.mkdir(parents=True, exist_ok=True)\n    result_folders = find_result_folders(base, pattern_name)\n    if not result_folders:\n        print(f\"‚ö†Ô∏è No folders matching '*dataset' found in '{output_base_path}'.\")\n        return\n    print(f\"üîç Found {len(result_folders)} result folder(s) to zip.\")\n    successful = sum(1 for folder in result_folders if zip_folder(folder, base))\n    print(f\"\\n‚úÖ DONE! Successfully zipped {successful} out of {len(result_folders)} folder(s).\")\n\nif __name__ == \"__main__\":\n    try:\n        output_root = os.getenv(\"OUTPUT_PATH\") or globals().get(\"OUTPUT_PATH\")\n        if not output_root:\n            raise ValueError(\"OUTPUT_PATH not defined\")\n        zip_stats_results_folders(\n            output_base_path=OUTPUT_PATH,\n            pattern_name=\"my_dataset\")\n    except Exception as e:\n        print(f\"‚ùå An error occurred: {e}\")","metadata":{"id":"5868b20b","papermill":{"duration":0.031197,"end_time":"2025-12-30T18:55:25.126961","exception":false,"start_time":"2025-12-30T18:55:25.095764","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}